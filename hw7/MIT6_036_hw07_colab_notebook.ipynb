{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  }
 },
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_xIaEwCD406A",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "#MIT 6.036 Spring 2019: Homework 7#\n",
    "\n",
    "This colab notebook provides code and a framework for problem 2 of [the homework](https://openlearninglibrary.mit.edu/courses/course-v1:MITx+6.036+1T2019/courseware/Week7/week7_homework/).  You can work out your solutions here, then submit your results back on the homework page when ready.\n",
    "\n",
    "## <section>**Setup**</section>\n",
    "\n",
    "First, download the code distribution for this homework that contains test cases and helper functions.\n",
    "\n",
    "Run the next code block to download and import the code for this lab.\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "2YM-_zLf9Bp-",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 408
    },
    "outputId": "849dbafd-f0a6-49a4-fcd8-b1eb935e85e9",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "source": [
    "from   hw7code import *\n",
    "import numpy as np\n",
    "import  modules_disp as disp"
   ],
   "execution_count": 23,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xFxhrJ5XDlvb",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# 2) Implementing Neural Networks\n",
    "\n",
    "This homework considers neural networks with multiple layers. Each layer has multiple inputs and outputs, and can be broken down into two parts:\n",
    "\n",
    "<br>\n",
    "A linear module that implements a linear transformation:     $ z_j = (\\sum^{m}_{i=1} x_i W_{i,j}) + {W_0}_jz$\n",
    "\n",
    "specified by a weight matrix $W$ and a bias vector $W_0$. The output is $[z_1, \\ldots, z_n]^T$\n",
    "\n",
    "<br>\n",
    "An activation module that applies an activation function to the outputs of the linear module for some activation function $f$, such as Tanh or ReLU in the hidden layers or Softmax (see below) at the output layer. We write the output as: $[f(z_1), \\ldots, f(z_m)]^T$, although technically, for some activation functions such as softmax, each output will depend on all the $z_i$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MjQgtwPHj08n",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "We'll use the modular implementation that we guided you through in the previous problem, which leads to clean code. The basic framework for SGD training is given below. We can construct a network and train it as follows:\n",
    "\n",
    "```\n",
    "# build a 3-layer network\n",
    "net = Sequential([Linear(2,3), Tanh(),\n",
    "                  Linear(3,3), Tanh(),\n",
    "    \t          Linear(3,2), SoftMax()])\n",
    "# train the network on data and labels\n",
    "net.sgd(X, Y)\n",
    "```\n",
    "Please fill in any unimplemented methods below:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cEwpgsbnho9K",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Linear Modules: ##\n",
    "Each linear module has a forward method that takes in a batch of activations A (from the previous layer) and returns a batch of pre-activations Z.\n",
    "\n",
    "Each linear module has a backward method that takes in dLdZ and returns dLdA. This module also computes and stores dLdW and dLdW0, the gradients with respect to the weights."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "-VsYLAxCfy7U",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "source": [
    "class Linear(Module):\n",
    "    def __init__(self, m, n):\n",
    "        self.m, self.n = (m, n)  # (in size, out size)\n",
    "        self.W0 = np.zeros([self.n, 1])  # (n x 1)\n",
    "        self.W = np.random.normal(0, 1.0 * m ** (-.5), [m, n])  # (m x n)\n",
    "\n",
    "    def forward(self, A):\n",
    "        self.A = A   # (m x b)  Hint: make sure you understand what b stands for\n",
    "        return  self.W.T@A+self.W0 # Your code (n x b)\n",
    "\n",
    "    def backward(self, dLdZ):  # dLdZ is (n x b), uses stored self.A\n",
    "        self.dLdW  = self.A @ dLdZ.T  # Your code\n",
    "        self.dLdW0 =  dLdZ@np.ones((self.A.shape[1],1)) # Your code\n",
    "        return self.W@dLdZ        # Your code: return dLdA (m x b)\n",
    "\n",
    "    def sgd_step(self, lrate):  # Gradient descent step\n",
    "        self.W  = self.W-lrate*self.dLdW # Your code\n",
    "        self.W0 = self.W0-lrate*self.dLdW0 # Your code"
   ],
   "execution_count": 24,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cqZ7_kZYr5s5",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    " You are encouraged to make your own tests for each module. A unit test method and an example test case are given below for your reference:"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "aY3yePY0r4eA",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "source": [
    "np.random.seed(0)\n",
    "\n",
    "# data\n",
    "X, Y = super_simple_separable()\n",
    "\n",
    "# module\n",
    "linear_1 = Linear(2, 3)\n",
    "\n",
    "#hyperparameters\n",
    "lrate = 0.005\n",
    "\n",
    "# test case\n",
    "# forward\n",
    "z_1 = linear_1.forward(X)\n",
    "exp_z_1 =  np.array([[10.41750064, 6.91122168, 20.73366505, 22.8912344],\n",
    "                     [7.16872235, 3.48998746, 10.46996239, 9.9982611],\n",
    "                     [-2.07105455, 0.69413716, 2.08241149, 4.84966811]])\n",
    "unit_test(\"linear_forward\", exp_z_1, z_1)\n",
    "\n",
    "# backward\n",
    "dL_dz1 = np.array([[1.69467553e-09, -1.33530535e-06, 0.00000000e+00, -0.00000000e+00],\n",
    "                                     [-5.24547376e-07, 5.82459519e-04, -3.84805202e-10, 1.47943038e-09],\n",
    "                                     [-3.47063705e-02, 2.55611604e-01, -1.83538094e-02, 1.11838432e-04]])\n",
    "exp_dLdX = np.array([[-2.40194628e-02, 1.77064845e-01, -1.27021626e-02, 7.74006953e-05],\n",
    "                                    [2.39827939e-02, -1.75870737e-01, 1.26832126e-02, -7.72828555e-05]])\n",
    "dLdX = linear_1.backward(dL_dz1)\n",
    "unit_test(\"linear_backward\", exp_dLdX, dLdX)\n",
    "\n",
    "# sgd step\n",
    "linear_1.sgd_step(lrate)\n",
    "exp_linear_1_W = np.array([[1.2473734,  0.28294514,  0.68940437],\n",
    "                           [1.58455079, 1.32055711, -0.69218045]]),\n",
    "unit_test(\"linear_sgd_step_W\",  exp_linear_1_W,  linear_1.W)\n",
    "\n",
    "exp_linear_1_W0 = np.array([[6.66805339e-09],\n",
    "                            [-2.90968033e-06],\n",
    "                            [-1.01331631e-03]]),\n",
    "unit_test(\"linear_sgd_step_W0\", exp_linear_1_W0, linear_1.W0)\n"
   ],
   "execution_count": 25,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "linear_forward: OK\n",
      "linear_backward: OK\n",
      "linear_sgd_step_W: OK\n",
      "linear_sgd_step_W0: OK\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2ETL01mPsBz4",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "The following datasets are defined for your use:\n",
    "*  `super_simple_separable_through_origin()`\n",
    "*  `super_simple_separable()`\n",
    "*  `xor()`\n",
    "*  `xor_more()`\n",
    "*  `hard()`\n",
    "\n",
    "Further, a plotting function is defined for your usage in modules_disp.py, and can be called in the colab notebook as `disp.plot_nn()`.\n",
    "```\n",
    "def plot_nn(X, Y, nn):\n",
    "    \"\"\" Plot output of nn vs. data \"\"\"\n",
    "    def predict(x):\n",
    "        return nn.modules[-1].class_fun(nn.forward(x))[0]\n",
    "    xmin, ymin = np.min(X, axis=1)-1\n",
    "    xmax, ymax = np.max(X, axis=1)+1\n",
    "    nax = plot_objective_2d(lambda x: predict(x), xmin, xmax, ymin, ymax)\n",
    "    plot_data(X, Y, nax)\n",
    "    plt.show()```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4s70beWJh09h",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Activation functions: ##\n",
    "Each activation module has a forward method that takes in a batch of pre-activations Z and returns a batch of activations A.\n",
    "\n",
    "Each activation module has a backward method that takes in dLdA and returns dLdZ, with the exception of SoftMax, where we assume dLdZ is passed in."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kwaNAtLnhenT",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Tanh: ###"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "ff6eD3dnftiR",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "source": [
    "class Tanh(Module):            # Layer activation\n",
    "    def forward(self, Z):\n",
    "        self.A = np.tanh(Z)\n",
    "        return self.A\n",
    "\n",
    "    def backward(self, dLdA):    # Uses stored self.A\n",
    "        tmp=1-self.A**2\n",
    "        return dLdA*(1-self.A**2) # Your code: return dLdZ (?, b)"
   ],
   "execution_count": 26,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2FW7ocKRhcgY",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### ReLU: ###"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "1fm2KsLUfqdp",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "source": [
    "class ReLU(Module):              # Layer activation\n",
    "    def forward(self, Z):\n",
    "        self.A = np.where(Z>0,Z,0)           # Your code: (?, b)\n",
    "        return self.A\n",
    "\n",
    "    def backward(self, dLdA):    # uses stored self.A\n",
    "        return dLdA*np.where(self.A>0,1,0)      # Your code: return dLdZ (?, b)"
   ],
   "execution_count": 27,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZKtXuTQ0hSNO",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "###SoftMax: ###\n",
    "For `SoftMax.class_fun()`, given the column vector of class probabilities for each point (computed by Softmax), return a vector of the classes (integers) with the highest probability for each point."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "fqK-CJrnfn22",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "source": [
    "class SoftMax(Module):           # Output activation\n",
    "    def forward(self, Z):\n",
    "        ypred=np.exp(Z)\n",
    "        Ypred=ypred/np.sum(np.exp(Z),axis=0)\n",
    "        return Ypred             # Your code: (?, b)\n",
    "\n",
    "    def backward(self, dLdZ):    # Assume that dLdZ is passed in\n",
    "        return dLdZ\n",
    "\n",
    "    def class_fun(self, Ypred):  # Return class indices\n",
    "        return  np.argmax(Ypred,axis=0)                 # Your code: (1, b)\n"
   ],
   "execution_count": 38,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CZc7HnMSh4fn",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Loss Functions:##\n",
    "Each loss module has a forward method that takes in a batch of predictions Ypred (from the previous layer) and labels Y and returns a scalar loss value.\n",
    "\n",
    "The NLL module has a backward method that returns dLdZ, the gradient with respect to the preactivation to SoftMax (note: not the activation!), since we are always pairing SoftMax activation with NLL loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "l4uy0pHVhNd8",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### NLL: ###"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "17Fb8mimflgb",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "source": [
    "class NLL(Module):       # Loss\n",
    "    def forward(self, Ypred, Y):\n",
    "        self.Ypred = Ypred\n",
    "        self.Y = Y\n",
    "        return -np.sum(Y*np.log(Ypred))    # Your code\n",
    "\n",
    "    def backward(self):  # Use stored self.Ypred, self.Y\n",
    "        return self.Ypred-self.Y      # Your code"
   ],
   "execution_count": 39,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "y1EffzDFkqMX",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Activation and Loss Test Cases: ##\n",
    "Run Test 1 and Test 2 below and compare your outputs with the expected outputs.\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "9DJFzpahkvcD",
    "outputId": "f37fe4f7-9d34-474f-cac3-2183396e7bed",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 180
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "source": [
    "# TEST 1: sgd_test for Tanh activation and SoftMax output\n",
    "np.random.seed(0)\n",
    "sgd_test(Sequential([Linear(2,3), Tanh(), Linear(3,2), SoftMax()], NLL()), test_1_values)"
   ],
   "execution_count": 40,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "linear_1.W: OK\n",
      "linear_1.W0: OK\n",
      "linear_2.W: OK\n",
      "linear_2.W0: OK\n",
      "z_1: OK\n",
      "a_1: OK\n",
      "z_2: OK\n",
      "a_2: OK\n",
      "loss: OK\n",
      "dloss: OK\n",
      "dL_dz2: OK\n",
      "dL_da1: OK\n",
      "dL_dz1: OK\n",
      "dL_dX: OK\n",
      "updated_linear_1.W: OK\n",
      "updated_linear_1.W0: OK\n",
      "updated_linear_2.W: OK\n",
      "updated_linear_2.W0: OK\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "Bd0dXg-Qk05_",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "source": [
    "# TEST 2: sgd_test for ReLU activation and SoftMax output\n",
    "np.random.seed(0)\n",
    "sgd_test(Sequential([Linear(2,3), ReLU(), Linear(3,2), SoftMax()], NLL()), test_2_values)"
   ],
   "execution_count": 41,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "linear_1.W: OK\n",
      "linear_1.W0: OK\n",
      "linear_2.W: OK\n",
      "linear_2.W0: OK\n",
      "z_1: OK\n",
      "a_1: OK\n",
      "z_2: OK\n",
      "a_2: OK\n",
      "loss: OK\n",
      "dloss: OK\n",
      "dL_dz2: OK\n",
      "dL_da1: OK\n",
      "dL_dz1: OK\n",
      "dL_dX: OK\n",
      "updated_linear_1.W: OK\n",
      "updated_linear_1.W0: OK\n",
      "updated_linear_2.W: OK\n",
      "updated_linear_2.W0: OK\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-l5JgBU2iBCZ",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Neural Network: ##"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eXMGcdnXgiF3",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Implement SGD. Randomly pick a data point Xt, Yt by using np.random.randint to choose a random index into the data. Compute the predicted output Ypred for Xt with the forward method. Compute the loss for Ypred relative to Yt. Use the backward method to compute the gradients. Use the sgd_step method to change the weights. Repeat.\n",
    "\n",
    "We will (later) be generalizing SGD to operate on a \"mini-batch\" of data points instead of a single point. You should strive for an implementation of the forward, backward, and `class_fun` methods that works with batches of data. Note that when $b$ is mentioned as part of the shape of a matrix in the code, this $b$ refers to the number of points."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "ejO15Vr7fhKB",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "source": [
    "class Sequential:\n",
    "    def __init__(self, modules, loss):            # List of modules, loss module\n",
    "        self.modules = modules\n",
    "        self.loss = loss\n",
    "\n",
    "    def sgd(self, X, Y, iters=100, lrate=0.005):  # Train\n",
    "        D, N = X.shape\n",
    "        sum_loss=0\n",
    "        for it in range(iters):\n",
    "            rnd=np.random.randint(N) # Your code\n",
    "            Xt,yt=X[:,rnd:rnd+1],Y[:,rnd:rnd+1]\n",
    "            ypred=self.forward(Xt)\n",
    "            sum_loss+=self.loss.forward(ypred,yt)\n",
    "            self.backward(self.loss.backward())\n",
    "            self.sgd_step(lrate)\n",
    "            self.print_accuracy(it,X,Y,sum_loss)\n",
    "    def forward(self, Xt):                        # Compute Ypred\n",
    "        for m in self.modules: Xt = m.forward(Xt)\n",
    "        return Xt\n",
    "\n",
    "    def backward(self, delta):                    # Update dLdW and dLdW0\n",
    "        # Note reversed list of modules\n",
    "        for m in self.modules[::-1]: delta = m.backward(delta)\n",
    "\n",
    "    def sgd_step(self, lrate):                    # Gradient descent step\n",
    "        for m in self.modules: m.sgd_step(lrate)\n",
    "\n",
    "    def print_accuracy(self, it, X, Y, cur_loss, every=250):\n",
    "        # Utility method to print accuracy on full dataset, should\n",
    "        # improve over time when doing SGD. Also prints current loss,\n",
    "        # which should decrease over time. Call this on each iteration\n",
    "        # of SGD!\n",
    "        if it % every == 1:\n",
    "            cf = self.modules[-1].class_fun\n",
    "            acc = np.mean(cf(self.forward(X)) == cf(Y))\n",
    "            print('Iteration =', it, '\\tAcc =', acc, '\\tLoss =', cur_loss, flush=True)"
   ],
   "execution_count": 50,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HUojaXqphDjh",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Neural Network / SGD Test Cases: ##\n",
    "Use Test 3 and Test 4 to help you debug."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "wmupM8OScodw",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "source": [
    "# TEST 3: you should achieve 100% accuracy on the hard dataset (note that we provided plotting code)\n",
    "X, Y = hard()\n",
    "nn = Sequential([Linear(2, 10), ReLU(), Linear(10, 10), ReLU(), Linear(10,2), SoftMax()], NLL())\n",
    "disp.classify(X, Y, nn, it=100000)"
   ],
   "execution_count": 51,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration = 1 \tAcc = 0.65 \tLoss = 0.9974639119740931\n",
      "Iteration = 251 \tAcc = 0.9 \tLoss = 88.6896277192825\n",
      "Iteration = 501 \tAcc = 0.9 \tLoss = 158.2826746183619\n",
      "Iteration = 751 \tAcc = 0.95 \tLoss = 224.50342136839265\n",
      "Iteration = 1001 \tAcc = 0.95 \tLoss = 286.3057226447637\n",
      "Iteration = 1251 \tAcc = 0.95 \tLoss = 342.92079359300914\n",
      "Iteration = 1501 \tAcc = 0.95 \tLoss = 394.6977925914093\n",
      "Iteration = 1751 \tAcc = 0.95 \tLoss = 439.2929966830097\n",
      "Iteration = 2001 \tAcc = 0.95 \tLoss = 473.18880179065684\n",
      "Iteration = 2251 \tAcc = 0.95 \tLoss = 519.4664650777934\n",
      "Iteration = 2501 \tAcc = 0.95 \tLoss = 563.6763321950756\n",
      "Iteration = 2751 \tAcc = 0.95 \tLoss = 607.7353879081847\n",
      "Iteration = 3001 \tAcc = 0.95 \tLoss = 659.8076805650754\n",
      "Iteration = 3251 \tAcc = 0.9 \tLoss = 688.5772472182229\n",
      "Iteration = 3501 \tAcc = 0.95 \tLoss = 732.3853460311528\n",
      "Iteration = 3751 \tAcc = 0.95 \tLoss = 775.4055120275076\n",
      "Iteration = 4001 \tAcc = 0.95 \tLoss = 817.4519956841236\n",
      "Iteration = 4251 \tAcc = 0.95 \tLoss = 856.6179438155071\n",
      "Iteration = 4501 \tAcc = 0.95 \tLoss = 904.1225835263764\n",
      "Iteration = 4751 \tAcc = 0.95 \tLoss = 945.5075132776586\n",
      "Iteration = 5001 \tAcc = 0.95 \tLoss = 990.7034037369245\n",
      "Iteration = 5251 \tAcc = 0.95 \tLoss = 1029.5044992471614\n",
      "Iteration = 5501 \tAcc = 0.95 \tLoss = 1070.6897350779202\n",
      "Iteration = 5751 \tAcc = 0.95 \tLoss = 1103.8921909888888\n",
      "Iteration = 6001 \tAcc = 0.95 \tLoss = 1144.9735561155883\n",
      "Iteration = 6251 \tAcc = 0.95 \tLoss = 1171.9032519438715\n",
      "Iteration = 6501 \tAcc = 0.95 \tLoss = 1214.3402208804446\n",
      "Iteration = 6751 \tAcc = 0.95 \tLoss = 1254.139978513193\n",
      "Iteration = 7001 \tAcc = 0.95 \tLoss = 1296.5068912182799\n",
      "Iteration = 7251 \tAcc = 0.95 \tLoss = 1344.6941645828665\n",
      "Iteration = 7501 \tAcc = 0.95 \tLoss = 1383.4738338491586\n",
      "Iteration = 7751 \tAcc = 0.95 \tLoss = 1421.744805712634\n",
      "Iteration = 8001 \tAcc = 0.95 \tLoss = 1452.492550058863\n",
      "Iteration = 8251 \tAcc = 0.95 \tLoss = 1490.8762810604003\n",
      "Iteration = 8501 \tAcc = 0.95 \tLoss = 1529.7346612393283\n",
      "Iteration = 8751 \tAcc = 0.95 \tLoss = 1567.9256292848838\n",
      "Iteration = 9001 \tAcc = 0.95 \tLoss = 1604.433285892348\n",
      "Iteration = 9251 \tAcc = 0.95 \tLoss = 1639.5477364634105\n",
      "Iteration = 9501 \tAcc = 0.95 \tLoss = 1676.7825952514477\n",
      "Iteration = 9751 \tAcc = 0.95 \tLoss = 1714.1687149589882\n",
      "Iteration = 10001 \tAcc = 0.95 \tLoss = 1742.8759012163086\n",
      "Iteration = 10251 \tAcc = 0.95 \tLoss = 1782.997287977755\n",
      "Iteration = 10501 \tAcc = 0.95 \tLoss = 1817.3396629127574\n",
      "Iteration = 10751 \tAcc = 0.95 \tLoss = 1863.6057725292014\n",
      "Iteration = 11001 \tAcc = 0.95 \tLoss = 1898.5980590702407\n",
      "Iteration = 11251 \tAcc = 0.95 \tLoss = 1923.6154456733398\n",
      "Iteration = 11501 \tAcc = 0.95 \tLoss = 1957.214450576121\n",
      "Iteration = 11751 \tAcc = 0.95 \tLoss = 1994.103425738779\n",
      "Iteration = 12001 \tAcc = 0.95 \tLoss = 2032.1854090710906\n",
      "Iteration = 12251 \tAcc = 0.95 \tLoss = 2065.656902875221\n",
      "Iteration = 12501 \tAcc = 0.95 \tLoss = 2095.019674222175\n",
      "Iteration = 12751 \tAcc = 0.95 \tLoss = 2136.4449748960283\n",
      "Iteration = 13001 \tAcc = 0.95 \tLoss = 2180.6133137996085\n",
      "Iteration = 13251 \tAcc = 0.95 \tLoss = 2216.571547423965\n",
      "Iteration = 13501 \tAcc = 0.95 \tLoss = 2250.5746066002334\n",
      "Iteration = 13751 \tAcc = 0.95 \tLoss = 2289.2851729459476\n",
      "Iteration = 14001 \tAcc = 0.95 \tLoss = 2320.743540643176\n",
      "Iteration = 14251 \tAcc = 0.95 \tLoss = 2353.763194249097\n",
      "Iteration = 14501 \tAcc = 0.95 \tLoss = 2384.3885510957853\n",
      "Iteration = 14751 \tAcc = 0.95 \tLoss = 2413.30046495185\n",
      "Iteration = 15001 \tAcc = 0.95 \tLoss = 2435.828787920673\n",
      "Iteration = 15251 \tAcc = 0.95 \tLoss = 2470.640653280827\n",
      "Iteration = 15501 \tAcc = 0.95 \tLoss = 2494.5414601489456\n",
      "Iteration = 15751 \tAcc = 0.95 \tLoss = 2527.9617200559446\n",
      "Iteration = 16001 \tAcc = 0.95 \tLoss = 2558.338422471328\n",
      "Iteration = 16251 \tAcc = 0.95 \tLoss = 2582.177227823163\n",
      "Iteration = 16501 \tAcc = 0.95 \tLoss = 2617.983657899315\n",
      "Iteration = 16751 \tAcc = 0.95 \tLoss = 2644.82632392549\n",
      "Iteration = 17001 \tAcc = 0.95 \tLoss = 2670.3119495576234\n",
      "Iteration = 17251 \tAcc = 0.95 \tLoss = 2700.2613931534015\n",
      "Iteration = 17501 \tAcc = 0.95 \tLoss = 2731.2305133201094\n",
      "Iteration = 17751 \tAcc = 0.95 \tLoss = 2749.80680183415\n",
      "Iteration = 18001 \tAcc = 0.95 \tLoss = 2777.9780233446977\n",
      "Iteration = 18251 \tAcc = 0.95 \tLoss = 2808.233483259099\n",
      "Iteration = 18501 \tAcc = 0.95 \tLoss = 2830.9604495745525\n",
      "Iteration = 18751 \tAcc = 0.95 \tLoss = 2859.1127226042854\n",
      "Iteration = 19001 \tAcc = 0.95 \tLoss = 2888.1297763400557\n",
      "Iteration = 19251 \tAcc = 0.95 \tLoss = 2914.270612700672\n",
      "Iteration = 19501 \tAcc = 0.95 \tLoss = 2941.574904010074\n",
      "Iteration = 19751 \tAcc = 0.95 \tLoss = 2976.2662613118596\n",
      "Iteration = 20001 \tAcc = 0.95 \tLoss = 2995.179798002328\n",
      "Iteration = 20251 \tAcc = 0.95 \tLoss = 3021.76627924716\n",
      "Iteration = 20501 \tAcc = 0.95 \tLoss = 3045.4564479489754\n",
      "Iteration = 20751 \tAcc = 0.95 \tLoss = 3069.9415084122743\n",
      "Iteration = 21001 \tAcc = 0.95 \tLoss = 3090.4381317303805\n",
      "Iteration = 21251 \tAcc = 0.95 \tLoss = 3117.802951637579\n",
      "Iteration = 21501 \tAcc = 0.95 \tLoss = 3144.302452710413\n",
      "Iteration = 21751 \tAcc = 1.0 \tLoss = 3176.070256637805\n",
      "Iteration = 22001 \tAcc = 0.95 \tLoss = 3204.501820400679\n",
      "Iteration = 22251 \tAcc = 0.95 \tLoss = 3237.7440921543903\n",
      "Iteration = 22501 \tAcc = 0.95 \tLoss = 3264.646293713657\n",
      "Iteration = 22751 \tAcc = 1.0 \tLoss = 3293.7574288735755\n",
      "Iteration = 23001 \tAcc = 0.95 \tLoss = 3314.370691937078\n",
      "Iteration = 23251 \tAcc = 0.95 \tLoss = 3336.2582379443097\n",
      "Iteration = 23501 \tAcc = 0.95 \tLoss = 3359.8567191744373\n",
      "Iteration = 23751 \tAcc = 1.0 \tLoss = 3388.2310873672873\n",
      "Iteration = 24001 \tAcc = 0.95 \tLoss = 3412.6146260465666\n",
      "Iteration = 24251 \tAcc = 0.95 \tLoss = 3440.7085232145128\n",
      "Iteration = 24501 \tAcc = 0.95 \tLoss = 3464.0949150467513\n",
      "Iteration = 24751 \tAcc = 0.95 \tLoss = 3480.1423186771563\n",
      "Iteration = 25001 \tAcc = 0.95 \tLoss = 3506.2149964427863\n",
      "Iteration = 25251 \tAcc = 0.95 \tLoss = 3527.9236867405784\n",
      "Iteration = 25501 \tAcc = 1.0 \tLoss = 3548.947742611662\n",
      "Iteration = 25751 \tAcc = 0.95 \tLoss = 3568.5299651760233\n",
      "Iteration = 26001 \tAcc = 0.95 \tLoss = 3587.6944054687956\n",
      "Iteration = 26251 \tAcc = 1.0 \tLoss = 3609.1827161530255\n",
      "Iteration = 26501 \tAcc = 1.0 \tLoss = 3629.37514865141\n",
      "Iteration = 26751 \tAcc = 0.95 \tLoss = 3644.698187937679\n",
      "Iteration = 27001 \tAcc = 0.95 \tLoss = 3660.992291635254\n",
      "Iteration = 27251 \tAcc = 1.0 \tLoss = 3680.288410785763\n",
      "Iteration = 27501 \tAcc = 1.0 \tLoss = 3696.684163906623\n",
      "Iteration = 27751 \tAcc = 0.95 \tLoss = 3717.1563195946087\n",
      "Iteration = 28001 \tAcc = 1.0 \tLoss = 3737.6651068976016\n",
      "Iteration = 28251 \tAcc = 0.95 \tLoss = 3757.5996179399312\n",
      "Iteration = 28501 \tAcc = 0.95 \tLoss = 3777.2715283982393\n",
      "Iteration = 28751 \tAcc = 0.95 \tLoss = 3791.9085222020694\n",
      "Iteration = 29001 \tAcc = 0.95 \tLoss = 3805.908375347272\n",
      "Iteration = 29251 \tAcc = 1.0 \tLoss = 3825.7758289456374\n",
      "Iteration = 29501 \tAcc = 0.95 \tLoss = 3849.587256199062\n",
      "Iteration = 29751 \tAcc = 1.0 \tLoss = 3870.0763992987027\n",
      "Iteration = 30001 \tAcc = 0.95 \tLoss = 3887.417696662365\n",
      "Iteration = 30251 \tAcc = 0.95 \tLoss = 3907.0881220834754\n",
      "Iteration = 30501 \tAcc = 0.95 \tLoss = 3920.7659013254997\n",
      "Iteration = 30751 \tAcc = 0.95 \tLoss = 3940.914257358964\n",
      "Iteration = 31001 \tAcc = 1.0 \tLoss = 3957.7681766830374\n",
      "Iteration = 31251 \tAcc = 1.0 \tLoss = 3970.6267926091655\n",
      "Iteration = 31501 \tAcc = 1.0 \tLoss = 3993.339757332735\n",
      "Iteration = 31751 \tAcc = 1.0 \tLoss = 4014.154068960374\n",
      "Iteration = 32001 \tAcc = 0.95 \tLoss = 4027.1986970394505\n",
      "Iteration = 32251 \tAcc = 1.0 \tLoss = 4042.2952698599365\n",
      "Iteration = 32501 \tAcc = 1.0 \tLoss = 4052.1132160962607\n",
      "Iteration = 32751 \tAcc = 1.0 \tLoss = 4067.4264701534466\n",
      "Iteration = 33001 \tAcc = 1.0 \tLoss = 4084.4910038099706\n",
      "Iteration = 33251 \tAcc = 1.0 \tLoss = 4102.029151981691\n",
      "Iteration = 33501 \tAcc = 1.0 \tLoss = 4116.107347783023\n",
      "Iteration = 33751 \tAcc = 0.95 \tLoss = 4132.400721025026\n",
      "Iteration = 34001 \tAcc = 0.95 \tLoss = 4149.004367601373\n",
      "Iteration = 34251 \tAcc = 1.0 \tLoss = 4166.191131174487\n",
      "Iteration = 34501 \tAcc = 1.0 \tLoss = 4181.074889156811\n",
      "Iteration = 34751 \tAcc = 0.95 \tLoss = 4199.684671829306\n",
      "Iteration = 35001 \tAcc = 0.95 \tLoss = 4215.797606835813\n",
      "Iteration = 35251 \tAcc = 0.95 \tLoss = 4232.324390098675\n",
      "Iteration = 35501 \tAcc = 1.0 \tLoss = 4250.881381216471\n",
      "Iteration = 35751 \tAcc = 1.0 \tLoss = 4265.017517791555\n",
      "Iteration = 36001 \tAcc = 0.95 \tLoss = 4279.510536035308\n",
      "Iteration = 36251 \tAcc = 1.0 \tLoss = 4294.2857721130395\n",
      "Iteration = 36501 \tAcc = 1.0 \tLoss = 4307.1257860375545\n",
      "Iteration = 36751 \tAcc = 1.0 \tLoss = 4321.269969230957\n",
      "Iteration = 37001 \tAcc = 1.0 \tLoss = 4331.6890922061575\n",
      "Iteration = 37251 \tAcc = 1.0 \tLoss = 4346.628178213668\n",
      "Iteration = 37501 \tAcc = 0.95 \tLoss = 4358.329766257261\n",
      "Iteration = 37751 \tAcc = 1.0 \tLoss = 4372.869996322828\n",
      "Iteration = 38001 \tAcc = 0.95 \tLoss = 4384.898985113414\n",
      "Iteration = 38251 \tAcc = 1.0 \tLoss = 4400.334157471425\n",
      "Iteration = 38501 \tAcc = 0.95 \tLoss = 4416.486626548904\n",
      "Iteration = 38751 \tAcc = 1.0 \tLoss = 4423.839656403422\n",
      "Iteration = 39001 \tAcc = 1.0 \tLoss = 4436.607812147044\n",
      "Iteration = 39251 \tAcc = 0.95 \tLoss = 4450.800634333233\n",
      "Iteration = 39501 \tAcc = 1.0 \tLoss = 4464.066446017502\n",
      "Iteration = 39751 \tAcc = 0.95 \tLoss = 4473.545919852068\n",
      "Iteration = 40001 \tAcc = 1.0 \tLoss = 4485.057534397973\n",
      "Iteration = 40251 \tAcc = 0.95 \tLoss = 4495.439400453913\n",
      "Iteration = 40501 \tAcc = 0.95 \tLoss = 4509.347443186381\n",
      "Iteration = 40751 \tAcc = 1.0 \tLoss = 4524.410194446465\n",
      "Iteration = 41001 \tAcc = 0.95 \tLoss = 4536.184024111887\n",
      "Iteration = 41251 \tAcc = 1.0 \tLoss = 4552.101337867963\n",
      "Iteration = 41501 \tAcc = 0.95 \tLoss = 4564.172226268377\n",
      "Iteration = 41751 \tAcc = 0.95 \tLoss = 4576.747209268781\n",
      "Iteration = 42001 \tAcc = 0.95 \tLoss = 4590.229436602767\n",
      "Iteration = 42251 \tAcc = 0.95 \tLoss = 4598.118394253876\n",
      "Iteration = 42501 \tAcc = 1.0 \tLoss = 4609.282823448201\n",
      "Iteration = 42751 \tAcc = 1.0 \tLoss = 4620.845372245336\n",
      "Iteration = 43001 \tAcc = 1.0 \tLoss = 4631.143035398383\n",
      "Iteration = 43251 \tAcc = 1.0 \tLoss = 4645.10627710073\n",
      "Iteration = 43501 \tAcc = 1.0 \tLoss = 4655.030473463024\n",
      "Iteration = 43751 \tAcc = 1.0 \tLoss = 4666.211641080721\n",
      "Iteration = 44001 \tAcc = 1.0 \tLoss = 4675.372324512817\n",
      "Iteration = 44251 \tAcc = 1.0 \tLoss = 4688.506053816683\n",
      "Iteration = 44501 \tAcc = 1.0 \tLoss = 4695.839105361458\n",
      "Iteration = 44751 \tAcc = 0.95 \tLoss = 4705.862884924151\n",
      "Iteration = 45001 \tAcc = 1.0 \tLoss = 4716.433255585913\n",
      "Iteration = 45251 \tAcc = 1.0 \tLoss = 4728.451104446107\n",
      "Iteration = 45501 \tAcc = 1.0 \tLoss = 4744.16020358635\n",
      "Iteration = 45751 \tAcc = 0.95 \tLoss = 4752.452314746485\n",
      "Iteration = 46001 \tAcc = 1.0 \tLoss = 4765.524061549733\n",
      "Iteration = 46251 \tAcc = 0.95 \tLoss = 4774.468587990891\n",
      "Iteration = 46501 \tAcc = 1.0 \tLoss = 4786.514489486233\n",
      "Iteration = 46751 \tAcc = 0.95 \tLoss = 4797.206202835969\n",
      "Iteration = 47001 \tAcc = 1.0 \tLoss = 4808.884515145376\n",
      "Iteration = 47251 \tAcc = 1.0 \tLoss = 4820.704438302901\n",
      "Iteration = 47501 \tAcc = 1.0 \tLoss = 4834.347475127478\n",
      "Iteration = 47751 \tAcc = 1.0 \tLoss = 4844.508581155614\n",
      "Iteration = 48001 \tAcc = 1.0 \tLoss = 4854.371806061976\n",
      "Iteration = 48251 \tAcc = 0.95 \tLoss = 4865.218635231041\n",
      "Iteration = 48501 \tAcc = 1.0 \tLoss = 4874.315529097259\n",
      "Iteration = 48751 \tAcc = 1.0 \tLoss = 4884.871666578036\n",
      "Iteration = 49001 \tAcc = 1.0 \tLoss = 4897.029121403631\n",
      "Iteration = 49251 \tAcc = 1.0 \tLoss = 4905.037040003436\n",
      "Iteration = 49501 \tAcc = 0.95 \tLoss = 4914.314296897056\n",
      "Iteration = 49751 \tAcc = 1.0 \tLoss = 4927.375616627088\n",
      "Iteration = 50001 \tAcc = 1.0 \tLoss = 4935.110436860826\n",
      "Iteration = 50251 \tAcc = 1.0 \tLoss = 4944.821765115787\n",
      "Iteration = 50501 \tAcc = 1.0 \tLoss = 4957.650187675993\n",
      "Iteration = 50751 \tAcc = 1.0 \tLoss = 4966.846944818647\n",
      "Iteration = 51001 \tAcc = 1.0 \tLoss = 4973.292126107787\n",
      "Iteration = 51251 \tAcc = 1.0 \tLoss = 4984.378355945372\n",
      "Iteration = 51501 \tAcc = 1.0 \tLoss = 4997.075960379689\n",
      "Iteration = 51751 \tAcc = 1.0 \tLoss = 5008.148919374535\n",
      "Iteration = 52001 \tAcc = 1.0 \tLoss = 5020.425074620073\n",
      "Iteration = 52251 \tAcc = 1.0 \tLoss = 5029.145155819596\n",
      "Iteration = 52501 \tAcc = 0.95 \tLoss = 5036.542553388556\n",
      "Iteration = 52751 \tAcc = 1.0 \tLoss = 5044.4806836743055\n",
      "Iteration = 53001 \tAcc = 1.0 \tLoss = 5051.041827866106\n",
      "Iteration = 53251 \tAcc = 1.0 \tLoss = 5061.417794134974\n",
      "Iteration = 53501 \tAcc = 1.0 \tLoss = 5073.115488923007\n",
      "Iteration = 53751 \tAcc = 1.0 \tLoss = 5082.536783544388\n",
      "Iteration = 54001 \tAcc = 1.0 \tLoss = 5095.089265748663\n",
      "Iteration = 54251 \tAcc = 1.0 \tLoss = 5101.341844254331\n",
      "Iteration = 54501 \tAcc = 1.0 \tLoss = 5108.587253560029\n",
      "Iteration = 54751 \tAcc = 1.0 \tLoss = 5116.478708460125\n",
      "Iteration = 55001 \tAcc = 1.0 \tLoss = 5124.022900479607\n",
      "Iteration = 55251 \tAcc = 1.0 \tLoss = 5133.622782179162\n",
      "Iteration = 55501 \tAcc = 1.0 \tLoss = 5143.392588401135\n",
      "Iteration = 55751 \tAcc = 1.0 \tLoss = 5153.96679927372\n",
      "Iteration = 56001 \tAcc = 1.0 \tLoss = 5160.066188813542\n",
      "Iteration = 56251 \tAcc = 1.0 \tLoss = 5170.52760334133\n",
      "Iteration = 56501 \tAcc = 1.0 \tLoss = 5176.385682218742\n",
      "Iteration = 56751 \tAcc = 1.0 \tLoss = 5183.23726220114\n",
      "Iteration = 57001 \tAcc = 1.0 \tLoss = 5193.323133414885\n",
      "Iteration = 57251 \tAcc = 1.0 \tLoss = 5202.5154394500405\n",
      "Iteration = 57501 \tAcc = 1.0 \tLoss = 5211.453602158757\n",
      "Iteration = 57751 \tAcc = 1.0 \tLoss = 5221.381823378047\n",
      "Iteration = 58001 \tAcc = 1.0 \tLoss = 5228.561890429381\n",
      "Iteration = 58251 \tAcc = 1.0 \tLoss = 5234.433300356707\n",
      "Iteration = 58501 \tAcc = 1.0 \tLoss = 5242.386345698498\n",
      "Iteration = 58751 \tAcc = 1.0 \tLoss = 5250.690810456797\n",
      "Iteration = 59001 \tAcc = 0.95 \tLoss = 5256.838092000945\n",
      "Iteration = 59251 \tAcc = 1.0 \tLoss = 5262.96894025502\n",
      "Iteration = 59501 \tAcc = 1.0 \tLoss = 5271.7149537836585\n",
      "Iteration = 59751 \tAcc = 1.0 \tLoss = 5279.45175510213\n",
      "Iteration = 60001 \tAcc = 1.0 \tLoss = 5285.839363778963\n",
      "Iteration = 60251 \tAcc = 1.0 \tLoss = 5293.613078060787\n",
      "Iteration = 60501 \tAcc = 1.0 \tLoss = 5299.782310557661\n",
      "Iteration = 60751 \tAcc = 1.0 \tLoss = 5308.771210811733\n",
      "Iteration = 61001 \tAcc = 1.0 \tLoss = 5317.286759648781\n",
      "Iteration = 61251 \tAcc = 1.0 \tLoss = 5323.275900559105\n",
      "Iteration = 61501 \tAcc = 1.0 \tLoss = 5329.666995769977\n",
      "Iteration = 61751 \tAcc = 1.0 \tLoss = 5333.037910349415\n",
      "Iteration = 62001 \tAcc = 1.0 \tLoss = 5337.413876543495\n",
      "Iteration = 62251 \tAcc = 1.0 \tLoss = 5343.007696316838\n",
      "Iteration = 62501 \tAcc = 1.0 \tLoss = 5350.505260363939\n",
      "Iteration = 62751 \tAcc = 1.0 \tLoss = 5356.140014677831\n",
      "Iteration = 63001 \tAcc = 1.0 \tLoss = 5363.427689526592\n",
      "Iteration = 63251 \tAcc = 1.0 \tLoss = 5368.851498149077\n",
      "Iteration = 63501 \tAcc = 1.0 \tLoss = 5374.162554622615\n",
      "Iteration = 63751 \tAcc = 1.0 \tLoss = 5379.487153455698\n",
      "Iteration = 64001 \tAcc = 1.0 \tLoss = 5384.9476164064845\n",
      "Iteration = 64251 \tAcc = 1.0 \tLoss = 5392.957858957552\n",
      "Iteration = 64501 \tAcc = 1.0 \tLoss = 5400.294555525115\n",
      "Iteration = 64751 \tAcc = 1.0 \tLoss = 5406.399691469815\n",
      "Iteration = 65001 \tAcc = 1.0 \tLoss = 5410.894292507332\n",
      "Iteration = 65251 \tAcc = 1.0 \tLoss = 5415.232772058267\n",
      "Iteration = 65501 \tAcc = 1.0 \tLoss = 5422.365641133471\n",
      "Iteration = 65751 \tAcc = 1.0 \tLoss = 5426.461403976423\n",
      "Iteration = 66001 \tAcc = 1.0 \tLoss = 5431.822874604528\n",
      "Iteration = 66251 \tAcc = 1.0 \tLoss = 5438.187242441457\n",
      "Iteration = 66501 \tAcc = 1.0 \tLoss = 5442.630772393726\n",
      "Iteration = 66751 \tAcc = 1.0 \tLoss = 5448.094623855565\n",
      "Iteration = 67001 \tAcc = 1.0 \tLoss = 5453.558650906694\n",
      "Iteration = 67251 \tAcc = 1.0 \tLoss = 5457.092844222306\n",
      "Iteration = 67501 \tAcc = 1.0 \tLoss = 5460.9446200615075\n",
      "Iteration = 67751 \tAcc = 1.0 \tLoss = 5465.760008230632\n",
      "Iteration = 68001 \tAcc = 1.0 \tLoss = 5470.968186026369\n",
      "Iteration = 68251 \tAcc = 1.0 \tLoss = 5476.186009493099\n",
      "Iteration = 68501 \tAcc = 1.0 \tLoss = 5481.451365928819\n",
      "Iteration = 68751 \tAcc = 1.0 \tLoss = 5487.736599094014\n",
      "Iteration = 69001 \tAcc = 1.0 \tLoss = 5492.132259694602\n",
      "Iteration = 69251 \tAcc = 1.0 \tLoss = 5497.678038181593\n",
      "Iteration = 69501 \tAcc = 1.0 \tLoss = 5504.63393931303\n",
      "Iteration = 69751 \tAcc = 1.0 \tLoss = 5510.142451451635\n",
      "Iteration = 70001 \tAcc = 1.0 \tLoss = 5515.272626739614\n",
      "Iteration = 70251 \tAcc = 1.0 \tLoss = 5518.9477630672445\n",
      "Iteration = 70501 \tAcc = 1.0 \tLoss = 5524.135307816202\n",
      "Iteration = 70751 \tAcc = 1.0 \tLoss = 5527.247662179193\n",
      "Iteration = 71001 \tAcc = 1.0 \tLoss = 5529.937470067512\n",
      "Iteration = 71251 \tAcc = 1.0 \tLoss = 5535.093480728255\n",
      "Iteration = 71501 \tAcc = 1.0 \tLoss = 5540.736907980377\n",
      "Iteration = 71751 \tAcc = 1.0 \tLoss = 5545.713408627914\n",
      "Iteration = 72001 \tAcc = 1.0 \tLoss = 5551.230526856282\n",
      "Iteration = 72251 \tAcc = 1.0 \tLoss = 5554.957230925358\n",
      "Iteration = 72501 \tAcc = 1.0 \tLoss = 5560.040819993354\n",
      "Iteration = 72751 \tAcc = 1.0 \tLoss = 5563.704427555121\n",
      "Iteration = 73001 \tAcc = 1.0 \tLoss = 5567.522262800341\n",
      "Iteration = 73251 \tAcc = 1.0 \tLoss = 5570.877622600839\n",
      "Iteration = 73501 \tAcc = 1.0 \tLoss = 5575.243904785035\n",
      "Iteration = 73751 \tAcc = 1.0 \tLoss = 5579.273325602176\n",
      "Iteration = 74001 \tAcc = 1.0 \tLoss = 5583.153079592075\n",
      "Iteration = 74251 \tAcc = 1.0 \tLoss = 5586.8880155710585\n",
      "Iteration = 74501 \tAcc = 1.0 \tLoss = 5591.520351267544\n",
      "Iteration = 74751 \tAcc = 1.0 \tLoss = 5595.214545859363\n",
      "Iteration = 75001 \tAcc = 1.0 \tLoss = 5598.930964430558\n",
      "Iteration = 75251 \tAcc = 1.0 \tLoss = 5602.836205493405\n",
      "Iteration = 75501 \tAcc = 1.0 \tLoss = 5607.334674962015\n",
      "Iteration = 75751 \tAcc = 1.0 \tLoss = 5610.943772881099\n",
      "Iteration = 76001 \tAcc = 1.0 \tLoss = 5614.293339857522\n",
      "Iteration = 76251 \tAcc = 1.0 \tLoss = 5617.49421239309\n",
      "Iteration = 76501 \tAcc = 1.0 \tLoss = 5620.914148653677\n",
      "Iteration = 76751 \tAcc = 1.0 \tLoss = 5623.653460987431\n",
      "Iteration = 77001 \tAcc = 1.0 \tLoss = 5627.670272202173\n",
      "Iteration = 77251 \tAcc = 1.0 \tLoss = 5630.94065794142\n",
      "Iteration = 77501 \tAcc = 1.0 \tLoss = 5633.809325371875\n",
      "Iteration = 77751 \tAcc = 1.0 \tLoss = 5637.103018110981\n",
      "Iteration = 78001 \tAcc = 1.0 \tLoss = 5641.3011880020895\n",
      "Iteration = 78251 \tAcc = 1.0 \tLoss = 5643.949902619791\n",
      "Iteration = 78501 \tAcc = 1.0 \tLoss = 5647.150259842698\n",
      "Iteration = 78751 \tAcc = 1.0 \tLoss = 5650.5423034023315\n",
      "Iteration = 79001 \tAcc = 1.0 \tLoss = 5652.760725542056\n",
      "Iteration = 79251 \tAcc = 1.0 \tLoss = 5656.520181579146\n",
      "Iteration = 79501 \tAcc = 1.0 \tLoss = 5658.9484454712365\n",
      "Iteration = 79751 \tAcc = 1.0 \tLoss = 5662.5458481738815\n",
      "Iteration = 80001 \tAcc = 1.0 \tLoss = 5665.687621763889\n",
      "Iteration = 80251 \tAcc = 1.0 \tLoss = 5668.934157415092\n",
      "Iteration = 80501 \tAcc = 1.0 \tLoss = 5672.588867223549\n",
      "Iteration = 80751 \tAcc = 1.0 \tLoss = 5674.671194055453\n",
      "Iteration = 81001 \tAcc = 1.0 \tLoss = 5676.8016166868265\n",
      "Iteration = 81251 \tAcc = 1.0 \tLoss = 5679.6342535050335\n",
      "Iteration = 81501 \tAcc = 1.0 \tLoss = 5682.379861539188\n",
      "Iteration = 81751 \tAcc = 1.0 \tLoss = 5685.538678517306\n",
      "Iteration = 82001 \tAcc = 1.0 \tLoss = 5688.425341020386\n",
      "Iteration = 82251 \tAcc = 1.0 \tLoss = 5691.168711427214\n",
      "Iteration = 82501 \tAcc = 1.0 \tLoss = 5693.865980217544\n",
      "Iteration = 82751 \tAcc = 1.0 \tLoss = 5696.565113602075\n",
      "Iteration = 83001 \tAcc = 1.0 \tLoss = 5699.140301240497\n",
      "Iteration = 83251 \tAcc = 1.0 \tLoss = 5701.719595005435\n",
      "Iteration = 83501 \tAcc = 1.0 \tLoss = 5703.843180930035\n",
      "Iteration = 83751 \tAcc = 1.0 \tLoss = 5705.966747107958\n",
      "Iteration = 84001 \tAcc = 1.0 \tLoss = 5708.90789734768\n",
      "Iteration = 84251 \tAcc = 1.0 \tLoss = 5710.839223739044\n",
      "Iteration = 84501 \tAcc = 1.0 \tLoss = 5713.514668579069\n",
      "Iteration = 84751 \tAcc = 1.0 \tLoss = 5716.21310892188\n",
      "Iteration = 85001 \tAcc = 1.0 \tLoss = 5718.0058213739585\n",
      "Iteration = 85251 \tAcc = 1.0 \tLoss = 5720.2572887195365\n",
      "Iteration = 85501 \tAcc = 1.0 \tLoss = 5723.169788439118\n",
      "Iteration = 85751 \tAcc = 1.0 \tLoss = 5725.713453417617\n",
      "Iteration = 86001 \tAcc = 1.0 \tLoss = 5728.01335024968\n",
      "Iteration = 86251 \tAcc = 1.0 \tLoss = 5730.559922559079\n",
      "Iteration = 86501 \tAcc = 1.0 \tLoss = 5732.719668038037\n",
      "Iteration = 86751 \tAcc = 1.0 \tLoss = 5734.6282803751155\n",
      "Iteration = 87001 \tAcc = 1.0 \tLoss = 5736.953870844442\n",
      "Iteration = 87251 \tAcc = 1.0 \tLoss = 5738.6984126721145\n",
      "Iteration = 87501 \tAcc = 1.0 \tLoss = 5740.634499633239\n",
      "Iteration = 87751 \tAcc = 1.0 \tLoss = 5742.624053429255\n",
      "Iteration = 88001 \tAcc = 1.0 \tLoss = 5744.673337178985\n",
      "Iteration = 88251 \tAcc = 1.0 \tLoss = 5746.781426599287\n",
      "Iteration = 88501 \tAcc = 1.0 \tLoss = 5749.344059229339\n",
      "Iteration = 88751 \tAcc = 1.0 \tLoss = 5751.566495594938\n",
      "Iteration = 89001 \tAcc = 1.0 \tLoss = 5753.409942711276\n",
      "Iteration = 89251 \tAcc = 1.0 \tLoss = 5755.421003509716\n",
      "Iteration = 89501 \tAcc = 1.0 \tLoss = 5758.01400924969\n",
      "Iteration = 89751 \tAcc = 1.0 \tLoss = 5759.735938817226\n",
      "Iteration = 90001 \tAcc = 1.0 \tLoss = 5761.288863097306\n",
      "Iteration = 90251 \tAcc = 1.0 \tLoss = 5762.667730243175\n",
      "Iteration = 90501 \tAcc = 1.0 \tLoss = 5764.415104516698\n",
      "Iteration = 90751 \tAcc = 1.0 \tLoss = 5766.197365221325\n",
      "Iteration = 91001 \tAcc = 1.0 \tLoss = 5768.549619116254\n",
      "Iteration = 91251 \tAcc = 1.0 \tLoss = 5770.426989767313\n",
      "Iteration = 91501 \tAcc = 1.0 \tLoss = 5771.8134598721645\n",
      "Iteration = 91751 \tAcc = 1.0 \tLoss = 5773.297921364265\n",
      "Iteration = 92001 \tAcc = 1.0 \tLoss = 5775.097452280989\n",
      "Iteration = 92251 \tAcc = 1.0 \tLoss = 5776.816426771921\n",
      "Iteration = 92501 \tAcc = 1.0 \tLoss = 5778.6059074594305\n",
      "Iteration = 92751 \tAcc = 1.0 \tLoss = 5780.168979455039\n",
      "Iteration = 93001 \tAcc = 1.0 \tLoss = 5781.970284567372\n",
      "Iteration = 93251 \tAcc = 1.0 \tLoss = 5783.695424071964\n",
      "Iteration = 93501 \tAcc = 1.0 \tLoss = 5785.6080852314735\n",
      "Iteration = 93751 \tAcc = 1.0 \tLoss = 5787.248407269663\n",
      "Iteration = 94001 \tAcc = 1.0 \tLoss = 5788.3768780611035\n",
      "Iteration = 94251 \tAcc = 1.0 \tLoss = 5789.861358968092\n",
      "Iteration = 94501 \tAcc = 1.0 \tLoss = 5791.890094772548\n",
      "Iteration = 94751 \tAcc = 1.0 \tLoss = 5793.279320572021\n",
      "Iteration = 95001 \tAcc = 1.0 \tLoss = 5794.720074211328\n",
      "Iteration = 95251 \tAcc = 1.0 \tLoss = 5796.885669835449\n",
      "Iteration = 95501 \tAcc = 1.0 \tLoss = 5798.864553818732\n",
      "Iteration = 95751 \tAcc = 1.0 \tLoss = 5800.185518416485\n",
      "Iteration = 96001 \tAcc = 1.0 \tLoss = 5802.019492492733\n",
      "Iteration = 96251 \tAcc = 1.0 \tLoss = 5803.260641033852\n",
      "Iteration = 96501 \tAcc = 1.0 \tLoss = 5805.184622205042\n",
      "Iteration = 96751 \tAcc = 1.0 \tLoss = 5806.71267267334\n",
      "Iteration = 97001 \tAcc = 1.0 \tLoss = 5808.39813563313\n",
      "Iteration = 97251 \tAcc = 1.0 \tLoss = 5809.776258745091\n",
      "Iteration = 97501 \tAcc = 1.0 \tLoss = 5811.22335135441\n",
      "Iteration = 97751 \tAcc = 1.0 \tLoss = 5812.60006869079\n",
      "Iteration = 98001 \tAcc = 1.0 \tLoss = 5814.244018803069\n",
      "Iteration = 98251 \tAcc = 1.0 \tLoss = 5815.294684537738\n",
      "Iteration = 98501 \tAcc = 1.0 \tLoss = 5816.718276183761\n",
      "Iteration = 98751 \tAcc = 1.0 \tLoss = 5818.457058522305\n",
      "Iteration = 99001 \tAcc = 1.0 \tLoss = 5819.7452219437355\n",
      "Iteration = 99251 \tAcc = 1.0 \tLoss = 5821.170107232669\n",
      "Iteration = 99501 \tAcc = 1.0 \tLoss = 5822.343608764688\n",
      "Iteration = 99751 \tAcc = 1.0 \tLoss = 5823.517384392948\n",
      "-3.46493986 -3.41956036 3.39710997 2.0597278\n"
     ]
    },
    {
     "data": {
      "text/plain": "<Figure size 640x480 with 2 Axes>",
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAgoAAAGTCAYAAABJQDpDAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjUuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8qNh9FAAAACXBIWXMAAA9hAAAPYQGoP6dpAAA89klEQVR4nO3dfXxU5Z3///dJMpkkQIIQCUECRFSkRVCTqkFQYjWKXQXXG/y5X0CFfktRXEzrKtLlbmux9Wa1VVB+cqO7othFlLZUjatyI7ICm4g3gOVGgyQRAiUDISSTmfP9I2ZkSCbk5MxNZub1fDzOI86Zc+b6XAbIJ9f1ua5jmKZpCgAAoBUJkQ4AAAB0XiQKAAAgIBIFAAAQEIkCAAAIiEQBAAAERKIAAAACIlEAAAABkSgAAICASBQAAEBAJAoAACAgEgUAAKLAunXrdMMNN6hPnz4yDENvvPHGae9Zu3at8vLylJKSorPPPlvPPfec5XZJFAAAiAK1tbUaNmyYnnnmmXZdv3fvXl1//fUaOXKkSktL9fDDD+u+++7TypUrLbVr8FAoAACii2EYWrVqlcaOHRvwmgcffFCrV6/W9u3bfeemTJmiTz75RB999FG720qyEygAAPHmxIkTamhosP05pmnKMAy/c06nU06n0/ZnS9JHH32koqIiv3PXXnutFi9eLLfbLYfD0a7PIVEAAKCdTpw4odz+XVV1wGP7s7p27apjx475nZs9e7bmzJlj+7MlqaqqSllZWX7nsrKy1NjYqOrqamVnZ7frc0gUAABop4aGBlUd8Gjv1v5K79bxMj/XUa9y877Wvn37lJ6e7jsfrNGEZqeOWDRXG5x6vi0kCgAAWJTeLcFWouD7nPR0v0QhmHr37q2qqiq/cwcOHFBSUpJ69uzZ7s8hUQAAwCKP6ZXHxlIAj+kNXjABFBQU6E9/+pPfuXfeeUf5+fntrk+QWB4JAIBlXpm2D6uOHTumsrIylZWVSWpa/lhWVqby8nJJ0owZMzRhwgTf9VOmTNHXX3+t4uJibd++XUuWLNHixYv1y1/+0lK7jCgAABAFtmzZosLCQt/r4uJiSdLEiRO1bNkyVVZW+pIGScrNzdWaNWt0//3369lnn1WfPn30+9//XjfffLOldtlHAQCAdnK5XMrIyFDFzr62ixn7DPpGNTU1IatRCBZGFAAAsMhjmvLY+D3bzr3hRo0CAAAIiBEFAAAs6mhB4sn3RwsSBQAALPLKlIdEAQAAtCaeRhSoUQAAAAExogAAgEXxtOqBRAEAAIu83x127o8WTD0AAICAGFEAAMAij81VD3buDTcSBQAALPKYsvn0yODFEmpMPQAAgIAYUQAAwKJ4KmYkUQAAwCKvDHlk2Lo/WjD1AAAAAmJEAQAAi7xm02Hn/mhBogAAgEUem1MPdu4NNxIFAAAsiqdEoVPXKJimKZfLJTOK9sQGACCWdOpE4ejRo8rIyNDRo0fD2q7b7dabb74pt9sd1nbDiT7GBvoYG+hj9PGahu0jWjD1AACARfE09RDSRGH+/Pl6/fXXtWPHDqWmpmr48OH67W9/q0GDBnXo865JuDXIEbbOkerQz5aP05juE+Wui43s91T0MTbY6ePbFWWhCSrIvI1OSY/I++1F8ibVRzqckKCP9iX0/lvQPxNNQjr1sHbtWt1zzz3atGmTSkpK1NjYqKKiItXW1oayWQAAQsqjBNtHtAjpiMJbb73l93rp0qXq1auXtm7dqiuuuCKUTQMAEDKmzToDkxqF1tXU1EiSevTo0er79fX1qq//fkjK5XJJaiqCcbvdcqQ6Qh+kJEdqkt/XWEQfY4OdProbncEOJyQaPU6/r7GIPtqX8F2RpMMRnp8T8cQww7T20DRNjRkzRn//+9+1fv36Vq+ZM2eO5s6d2+L88uXLlZaWFuoQAQBRbsyYMSH9fJfLpYyMDL3zaX916dbx6YPao14VXfC1ampqlJ6eHsQIgy9sicI999yjv/zlL9qwYYP69u3b6jWtjSjk5OSourpa6enpGtN9YjhClSM1SXcvvllLJq2Uu64xLG2GG32MDXb6uGrnthBFFVyNHqfeLZulqy+cp6TE2Cz0o4/2JWSVSgr9iEJzovDXbbm2E4XRQ/dGRaIQljHZadOmafXq1Vq3bl3AJEGSnE6nnM6Ww1IOh0MOhyPslevuusaYrZZvRh9jQ0f66Iiy6vqkxPqoi9kq+thxCUw5hExIEwXTNDVt2jStWrVKH3zwgXJzc0PZHAAAYeGVIa+NlQteRc+OwyFNFO655x4tX75cb775prp166aqqipJUkZGhlJTU0PZNAAAIcOGS0GycOFCSdKoUaP8zi9dulR33nlnKJsGACBkPGaCPGbHRxQ8UfQMo5BPPQAAgOgVuwvMAQAIkaYahY5PH9i5N9xIFAAAsMhrcxtmihkBRFy0PPQJQOdGogAAgEUUMwIAgIC8SoibfRSi5zmXAAAg7BhRAADAIo9pyGPjUdF27g03EgUAACzy2Fz14GHqAQAAxAJGFAAAsMhrJshrY9WDl1UPAADErniaeiBRAADAIq/sFSR6gxdKyFGjAAAAAmJEAQAAi+xvuBQ9v6eTKAAAYJH9LZyjJ1GInkgBAEDYMaIAAIBFXhnyyk4xIzszAgAQs5h6AAAAECMKAABYZn/Dpej5PZ1EAQAAi7ymIa+dDZei6OmR0ZPSAACAsGNEAQAAi7w2px7YcAkAgBhm/+mRJAoAAMQsjwx5bOyFYOfecIuelAYAAIQdIwoAAFjE1AMAAAjII3vTB57ghRJy0ZPSAACAsGNEAQAAi5h6AAAAAfFQKAAA0OksWLBAubm5SklJUV5entavX9/m9S+//LKGDRumtLQ0ZWdn66677tKhQ4cstUmiAACARaYMeW0cZgcKIVesWKHp06dr5syZKi0t1ciRIzV69GiVl5e3ev2GDRs0YcIETZo0SZ9//rn++Mc/avPmzZo8ebKldkkUAACwqHnqwc5h1ZNPPqlJkyZp8uTJGjx4sJ566inl5ORo4cKFrV6/adMmDRgwQPfdd59yc3M1YsQI/exnP9OWLVsstUuiAABAhLhcLr+jvr6+1esaGhq0detWFRUV+Z0vKirSxo0bW71n+PDh+uabb7RmzRqZpqlvv/1W//Vf/6Wf/OQnlmIkUQAAwKLmx0zbOSQpJydHGRkZvmP+/PmttlddXS2Px6OsrCy/81lZWaqqqmr1nuHDh+vll1/WuHHjlJycrN69e6t79+76wx/+YKmvrHoAAMAij82nRzbfu2/fPqWnp/vOO53ONu8zDP/aBtM0W5xr9sUXX+i+++7TrFmzdO2116qyslIPPPCApkyZosWLF7c7VhIFAAAsOnlUoKP3S1J6erpfohBIZmamEhMTW4weHDhwoMUoQ7P58+fr8ssv1wMPPCBJGjp0qLp06aKRI0fq17/+tbKzs9sVK1MPAAB0csnJycrLy1NJSYnf+ZKSEg0fPrzVe44fP66EBP8f84mJiZKaRiLaixEFAAAs8ipBXhu/a3fk3uLiYo0fP175+fkqKCjQokWLVF5erilTpkiSZsyYof379+ull16SJN1www366U9/qoULF/qmHqZPn65LLrlEffr0aXe7JAoAAFjkMQ15bEw9dOTecePG6dChQ5o3b54qKys1ZMgQrVmzRv3795ckVVZW+u2pcOedd+ro0aN65pln9Itf/ELdu3fXVVddpd/+9reW2iVRABAVVu45psw0twr7pCoxoeP/QAPRbOrUqZo6dWqr7y1btqzFuWnTpmnatGm22qRGAUCn9fa+Wl32+j5J0uS1B3TtXyo04OWvtHSHK8KRId4Fa3lkNGBEAUCnVPLNcd34VqUcSvU7X3Hco8lrD2h9ZZ2WFLZe7Q2Emmnz6ZEmD4UCAHv+5aNqNXoDv//il0d134YD4QsIiFMkCgA6nS0HT2jb4YbTXvfs5y59UHE8DBEB/jwybB/RIqSJwrp163TDDTeoT58+MgxDb7zxRiibAxAjvjnW2O5rn/ucegWEn9e0W6cQ6R60X0gThdraWg0bNkzPPPNMKJsBEGO8FjaDKTvU+kN0AARHSIsZR48erdGjR4eyCQAxaPOBE+2+NiUxeoZwETu8NosZ7dwbbp1q1UN9fb3fIzZdrqYhRbfbLbfbLUeqIyxxOFKT/L7GIvoYG9rqo7ux7YfLdGabDyQoNaFptcOpX081pl/3qO6rJDV6nH5fY1Go+5jgdkuSHI7w/JzwypDXRp2BnXvDzTCtbPhspyHD0KpVqzR27NiA18yZM0dz585tcX758uVKS0sLYXQAgFgwZsyYkH6+y+VSRkaG7njvDiV3Te7w5zQca9Dyq5arpqamXQ+FiqRO9avWjBkzVFxc7HvtcrmUk5OjoqIipaena0z3iWGJw5GapLsX36wlk1bKXdf+oqpoQh9jQ1t9XLVzW4Sisu/XWw/riW1HJDWNJCwZskR3f3a36rx1vmuSDOmN6/ro8t4pEYoyeBo9Tr1bNktXXzhPSYmxWXMR6j4mZJUG/TPRpFMlCk6ns9VncTscDjkcDrnr3GGNx13XGPY2w40+xobW+uhIit4fOJN/mKzHPz2hE57vBzzrvHV+icIrV2dpVF9DUvT281RJifVR/X1rj1D1MSFMUw7N4qlGIXoiBRA3cro69PKPs+QMUKg4K+8M3TawW5ijAr7nlc0tnKOoRiGkIwrHjh3Trl27fK/37t2rsrIy9ejRQ/369Qtl0wCi3Njcriq7JVnPf97022f/rkkadmZXTf1hhkZkt17YCCD4QpoobNmyRYWFhb7XzfUHEydObPUpV4hDpqmrtE/vKUcymjLsRNOrK/SN3jdIJuPded2T9eil3fTWVqns1n4xPyyP6GHaXPVgMqLQZNSoUQrTogpEgQKzQokytcE4q+mEaepxrdUwVesCHdTT5sVKlKlfaZNGqEL9TJdeNIZENmgAaIXdJ0Dy9EjgFAVmhX6lTTJk6jfmZdpgnKXfaZ2GqVqS9A/aK0NShuo1QhWSpP+jHfKYCfpP4wcRjBwA4huJAkJuoPl3/UqblKymRwE+rE36q5mrYTrod91PtNfvdZ0StU1nhi1OAGgvVj0AQbRb3VWi/r7XDpm6UXva/MNXp0T9SiO0zSBRAND52HsglL1pi3AjUUDoGYae0sX6i3JbfbtaLTfM+V9laZsyQx0ZAOA0SBQQHoah7erR4rQpKVMtHwB0uSr0z/pfiWJYAJ1Q87Me7BzRghoFhMW15le6X1tbnD/5r0qdEuWQV0lqSg7+QXtVpyQt0rAwRQkA7RNPqx4YUUDInWP+XfdrixJPOrdbGX7XmJL+XXl6RJep8bv0waVk/fdJtQ0A0FlQowAE0S7jDL2q832vV+g8TdHVfjULf9LZet/opw3GWXpEl+mwnPoXXaHdRvcIRAwAaMbUA8JimTFEMqUkefWCMVSS9JR5sSSpSml61Rjsu3aDcZa2mFk6YfDHE0DnFE9TD/xLjLBZduoui4ahp5TX6rUkCQA6s3hKFJh6ANqQYjbqAtN/Y6huZr0GmYcjFBEAhBeJAhBAitmoR7RBj2q9LjObtpXuZtbrMa3T77ROPzSrIxwhgEgxZW+JZDQt/CZRAFqRbHr0iDZoqKqVLK/+VZt0tfm1HtM6DVSN0tSo32iDfkCyAMQlVj0Acc6tBFWqi+91srx6UJs1UDW+c8fk0OFWdpUEgFhCxRjQCtMw9ISZL0m6Vl+3eP+AUvULXakqo2u4QwPQCVDMCECmYej/11DVtpJPv6rzSRKAOMbUAwB1M+v1W61TFzW2eG+KPvEVOAJALCNRAFqRYjb6ChebNZz016W5wPFi89tIhAcgwhhRAOLcCSNJW5Tle31AqfqpivT2Sc+eqFQX7TnlmRUA4oNpGraPaEExIxDAC8ZQyZQKtc9XuNhc4Hi+DuuXulJHDFY9APHI7qOiecw0ECNeMIbqNXOQXIZT0verIbrIrWNGcoSja9u1fS5s8/23K8rCEgeA6EaiAJxGc5LQzDQMHVPnThIAhFY8LY8kUQAAwCK7dQbRVKNAMSMAAAiIEQUAACxi6gEAAATE1AMAAIAYUQAAwDLT5tRDNI0okCgAAGCRKck07d0fLZh6AAAAATGiAACARV4ZMtjCGQAAtCaeVj2QKAA2tPW8hNM9awFA9PKahow42UeBGgUAABAQIwoAAFhkmjZXPUTRsgcSBQAALIqnGgWmHgAAQECMKAAAYFE8jSiQKAAxqq0VGQDsYdUDAACAGFEAgJjz5ZEGPfNZjf70da3qPaYuzHRqyg8ydOOALpEOLWaw6gEAEJX+Wl6rW0uqVNf4/U+it/cd19v7jmvy+el6/speEYwudjQlCnZqFIIYTIgx9QAAMeLQCY/GnZIknOyFHS4t2eEKc1SIdiQKABAjlu5wqTZAktBswec1YYomtjWverBzRAsSBQCIEesq6057TWl1vVwN3jBEE9vMIBzRghoFoA0sMUQ0Mdr5S2r0/C7becXTPgphGVFYsGCBcnNzlZKSory8PK1fvz4czQJAXCnsk3baa/LPdKpbMoPJaL+Q/2lZsWKFpk+frpkzZ6q0tFQjR47U6NGjVV5eHuqmASCu3Dmom9JPkwRMG9I9PMHEujiaewh5ovDkk09q0qRJmjx5sgYPHqynnnpKOTk5WrhwYaibBoC40t2ZqJVFvdXV0fqw9n1DMvR/zusW5qhilN1Cxg5OPVgdoa+vr9fMmTPVv39/OZ1ODRw4UEuWLLHUZkhrFBoaGrR161Y99NBDfueLioq0cePGFtfX19ervr7e99rlalrG43a75Xa75Uh1hDJcH0dqkt/XWEQf28fd6LTRfuj/vLbVRzuxdyaNHqff11gUzD6OzHKq9B+7aslOl94qr9MJj1dDezh19/npurJPqtyNtpvokFB/HxPcbkmSwxGenxOR0DxCv2DBAl1++eV6/vnnNXr0aH3xxRfq169fq/fcdttt+vbbb7V48WKdc845OnDggBobrf0hMEwzdNs+VFRU6KyzztKHH36o4cOH+87/5je/0YsvvqidO3f6XT9nzhzNnTu3xecsX75caWmnn3sDAMS3MWPGhPTzXS6XMjIylLt0phLSUjr8Od7jJ7T3rkdUU1Oj9PT0dt1z6aWX6uKLL/YbkR88eLDGjh2r+fPnt7j+rbfe0u233649e/aoR48eHY41LL9OGqeU4pqm2eKcJM2YMUPFxcW+1y6XSzk5OSoqKlJ6errGdJ8Y8lilpt/O7l58s5ZMWil3XYTS7xDrDH0806zVQaPLac91VDD6uGrntg63f9OgoR2+tz1W7dymRo9T75bN0tUXzlNSYv3pb4pC9DE2hLqPCVmlQf/MtgRr1UPzyHkzp9Mpp7PlqIvVEXpJWr16tfLz8/W73/1O//Ef/6EuXbroxhtv1L/9278pNTW13bGGNFHIzMxUYmKiqqqq/M4fOHBAWVlZLa4P9D/I4XDI4XDIXecOWaytcdc1hr3NcItUH88zD+tRrdc76q/njAslSReZ32qeNuplDdarxvlBa8tOHx1JHf8HLdT/X0+OLSmx3las0YA+xoZQ9TEhSqcccnJy/F7Pnj1bc+bMaXFddXW1PB5Pi5+dWVlZLX7GNtuzZ482bNiglJQUrVq1StXV1Zo6daoOHz5sqU4hpIlCcnKy8vLyVFJSoptuusl3vqSkJOTDQ+i8mpOEbnLrZu2STOl/lK152qgUeTRJn0mmgposAEBQ2ShI9N0vad++fX5TD639snyy9o7QS5LX65VhGHr55ZeVkZEhqWmBwS233KJnn3223aMKIZ96KC4u1vjx45Wfn6+CggItWrRI5eXlmjJlSqibRid1lo4pTd//tn2zdmmsdinxpGsG6kjTU1Pau4MMAIRRsJ4emZ6e3q4aBasj9JKUnZ2ts846y5ckSE01DaZp6ptvvtG5557brlhDvjxy3LhxeuqppzRv3jxdeOGFWrdundasWaP+/fuHuml0Uu8b/fSYfiTPSedOThI+UF/N1yUkCQA6rzDvo3DyCP3JSkpK/BYLnOzyyy9XRUWFjh075jv35ZdfKiEhQX379m1322HZnmvq1Kn66quvVF9fr61bt+qKK64IR7PoxP7b6K+VOq/F+f3qqvm6RF6DneMA4GTFxcV64YUXtGTJEm3fvl3333+/3wj9jBkzNGHCBN/1d9xxh3r27Km77rpLX3zxhdatW6cHHnhAd999d+cpZgQCucj8Vjdqd4vzZ+mY/q+26TldGJY47D7L4do+FwYlDgDRJRLPehg3bpwOHTqkefPmqbKyUkOGDPEboa+srPTb9bhr164qKSnRtGnTlJ+fr549e+q2227Tr3/9a0vtkigg7JpXN6T4TT58r7nAsXk1BAB0ShHYhnnq1KmaOnVqq+8tW7asxbnzzz+/xXSFVYzvIuwc8irhpL9hH6ivHlO+X9rgkM1KIQBAUJAoIOw+NrI1VwVqUIKvcPEdY4B+p0vkkbRaA/UHXUgxI4BOy85zHuxOW4QbUw+IiI+NbN1vjtIudfcVLr5n9FOF2UU71IMkAUDnZvcJkFE0YEqigIj50mi59/gOo2cEIgEABEKiAACAZcZ3h537owOJAgAAVsXR1APFjAAAICBGFAAAsCqORhRIFAAAsCpIT4+MBkw9RKkss1Y3mrv8zg0wa3SN+VVkAgKAONL89Eg7R7RgRCEKZZm1elxr1VvH1cV06xVjsAaYNfqd1ilD9UoyTf3VyI10mACAGECiEGVSzEY99l2SIEl363OdYdZrlPbpDNVLkqZrq2rMZG00zopkqJ3Gqp3b5Eiq79C9kXzok90HVgEIoTiqUWDqIcqcMJK0Suf6nbtJu3xJgiR9oZ4qVa9whwYA8aO5RsHOESVIFKLQKuNcLdCwVt/7TD31sEaoznCEOSoAQCxi6iFKlaqXapWkLmpscZ4kAQBCyzCbDjv3RwsShSjUXLh4apIgSeO1XW4zQa8YgyMQGQDECWoU0FmlmI16VOv9ahL2q4vfNXfrc11p7gt3aACAGESiEGVOGElapKHyfPdAkc/UU1N0jV/Nwib11odixQMAhEwcFTMy9RCF3jP6Sab0E+3RTI34fiWEKV2kA5qnAjUa5IAAEDJxNPVAohCl3jP66T0zRzK+z0pXGedqlXmO3zmEX6FZru3qoSqjq+/caHOvPlK2jhgpEYwMAKzj185o1lpCQJIQUdeaX+lBfawntFa9zWOSpNvMnSrWVj2utepunohwhACCwgzCESVIFIAgGWHu1/3aokRJvVSnJ7RWk81t+qk+lST111E9rrVKNj2RDRSAfSQKAKz6RJn6Shm+171Up3H60u+at5SrBiMx3KEBCDaKGYHo0NbzENyNTr21dVzLN6oapd0N0uVpvmc5nGUeVRe59aXRo8OxHDWcesC8Qo9pnQaqpsX7z2uo/ss4r8OfDwCRQKKAmLf5YL1e23NQh+u9OjshQXfPr1X/Lz0yl2ZLakoSHtdapcijh8yR2mkzWdho9mmRKLjk0Ab1sdUPAJ1HPO3MyNQDYlZto1eSVPTn/frDZzV6+W9H9W87azRwbKNmFZgy7qrU9eYePa61ytQJdZVbj2q9Bpp/73Cbt5k7NV7bW5xPl9uvwBFAlKNGAYh+U9cdbPW8N0H69ZXSwgtM3a//Vaa+X4lQoa769pSdLtur0Cz3FS42O6hU33/3Up1+p3UUMwKIKiQKiEk7jzToT1/XtnnNby+XvCfVE32pM/SgRuqYkdyhNj9SH21Tpu/18xqqn+lq7f6uwNEj6SX9kGJGAFGFRAExadXeY6cd2SvvLm0+qWzgNZ3X4SRBkhqUqJkaoW3K9BUu1ipZD+gKfanuelw/0rtG/w5/PoDOw9D3dQodOiLdAQsoZkRMqnW3bwKw9qS84AFt1jHToa1Gb8vt5ZtV+pm26WGN0AO6Ql4jQYVmuW7Rl3pIIzVNV8l7yrbaba3YAIDOghEFxKQf9Dj9yECiV8qp/n4awCmv5mqjBpuHLLWVb1ZprjZqgFx6XGvVUydUaJbrQW3WeTqix7ROXeS23AcAnVgc7aNAooCY9I+5XZWZ0nYtwD9UJeiho9frfeX4zn2hntqt7u1vyDR1lz5TsppWWPRRrf6g9/SgNivxu8mPgarRtfrKahcAdGasegCimzPR0NOXZwZ8P7tOeuKnfeQynHpUl+h95ahUZ+pXGmGt2NAwNFMj9JXSfad66oQvSZCkVTpH/2UM6lA/ACDSSBQQs67v17TMcdRZqb7CodQkQxPP66aN/9RPuX2bnuToNQw9qkusJwnfOWKk6AFdoSqltXjvrxqgBcaFHe0CgM4qjkYUKGZEzFtVlK0j7gwdafAoOy1JXR0t82OvYahBHV+2eJEO6EzVtTg/TAd1pnlcB42WSQSA6MXOjECMOTM1UedmJLeaJNjVXLiY2MqvCH1Uq8e1Vmeax4PeLoAIYkQB6ByCsYTwpkFD5a4L0aoD09QN2u2XJPxZuRqiQxogl6SmZKFAFVqtc0ITAwCEECMKgB2GoV9phL5Q04OkVukcPW3k6QFd4StwXKofarVBkgDEFEYUALTXccOhGeZIXauvtMo4V9J3BY7mFbpcFfqLcXab97savHr3m+M63mjqgp7JGtbTGY6wAdgQTzUKJApAEBw3HFqlc/3OHTFS9BcFThI8XlMzPz6khV/U6NhJO0kWZKXouSvO1JAeJAwAIo+pByBCJq89oMc+OeKXJEjSR9+eUOHq/fpbTUOEIouM3TVubais0x4Xu1giCsTRzoyMKAARcDTriF768mjA9w/Xe/XI//5dywqzwhhVZKyrqNOvNh/Sh1XfP+57ZO8U/fqSnhqRndrGnUAE2a0zYOoBaD87Kxuu7XNhwPccqQ79bHmHP9q2tvp1/4cH9T+ftX3/H3cf07MjzlSXECzp7CxKvjmuG/9aoQav//n1VSdU9JcK/Xl0tkZmMQUDRFLs/gsEdGL7jzee9poTHlOH6z1hiCYyTNPUvesPtkgSmtV7TE3bcDC8QQHtZOsR0zYLIcMtpInCI488ouHDhystLU3du3cPZVNAVMlKPf1gXnKCdIaz47tFdnbvV9Rp12nqEXYcceujb0+0eQ0QEXG0PDKkiUJDQ4NuvfVW/fznPw9lM0DUmXBet9Nec1Nu15DsJNlZ7KppX9HibhvFjTv+3qCPD5zQoROxOzIDhFpIaxTmzp0rSVq2bFkomwGizo96pejms7to5Z7aVt/v5jA08+IzwhxVeGUkty8JyuhAsrRi11HNL/27Pj3ctHIkOaEp8Xr00p7q181h+fOAFuxOH0TRiEKnKmasr69XfX2977XL1bQFrtvtltvtliM1PH/BHd8NCzvaMTwcrTpTH92NHS9Wa+vPRKT7eLp+Lbuyv3qnVOs/vzzqN08/uHuy/jDyTJ2X7pT7NKUMjR6n39docm1fh850Hm2xPPRkGckJGpXdXR8ean8fF22v0YObXJISlZrw/aqJ1Xs92nLgkN6+vo/6du1cyUI0fx/bK9R9THA3jTw5HGH63sbRqgfDNM2Qh7ts2TJNnz5dR44cafO6OXPm+EYhTrZ8+XKlpfH0PQBA28aMGRPSz3e5XMrIyNDZM3+jxJSUDn+O58QJ7XnkYdXU1Cg9PT2IEQaf5V+1Av0wP9nmzZuVn59vOZgZM2aouLjY99rlciknJ0dFRUVKT0/XmO4TLX9mRzhSk3T34pu1ZNJKuetOX50ejTpTH1ft3BbwvZsGDe3w50a6j231K1gaPU69WzZLV184T0mJ9ae/oZMxZWrulsNa8HmN3CeNqjgSpGlDuutf83q02UePaeqF7S4t3uHS39pZ8+BMNLTz9n7KSO48haLR/n1sj1D3MSGrNOifiSaWE4V7771Xt99+e5vXDBgwoEPBOJ1OOZ0th6UcDoccDkfongAYgLuuMexthltn6KMjKfA/GsGILVJ9bKtfwZaUWB/W9oLpkcu66t4LUrR811FVHveoT1qi/uncbspKS5L0fZ9O7aPHa+r2kiq98VXrdR6B1HmlirpaZaZ1vmH+aP4+tleo+pgQrimH7/CshzZkZmYqMzMzFLEAiFPZXZL0i2HWijeX7nRZThKapbezkPJAXaM8XikrLVEJRvRsuQsEU0irvMrLy3X48GGVl5fL4/GorKxMknTOOeeoa9euoWwaQIx7/gtXh+67pJdTA06z8uGlL116etsRlR1qWjWR2y1JP/9hhv75gu5KSiBhQHwJaaIwa9Ysvfjii77XF110kSTp/fff16hRo0LZNIAYV3bI+vC1Ienhi3q0ec2/fFStJ7Yd8Tu392ij/mXTIa2vPKGVRb2VSLKAOFr1ENLdXJYtWybTNFscJAkA7HImWvth3SXJ0KIre+mGAV0CXrPp2xMtkoST/enr2jYf5oX4EU9bOEd+ET1i3uke+tTWg52AQG7o30Wv7T7W5jU/PitVg89I1vndk/VP53Y7bW3Cc1/UnLbdRdtrdNf5nXs5GxBMJAoAotL0C7rr9b3H1BjgoVJnpiRqxTW9LT0v47PvdnJsyyeHTn8N4kQUjQrYEbsbyQOIaZdmpWjpqKxWpyB6pSbqL9dnW36oVmo7pjPSkqhPgOLqoVCMKACIWnec200/PitVL+xw6X++PaGkBEPX5aTpn87tpi4deEbEmAFdtPE0T6sc20aNAxCLSBQARLWstCTNvLjtlQztdff56Xr8kyM6GOBpk85EQ/98QfegtIXoFk8bLjH1AADf6ZGSqL/+pI+y01pOWXR1GFpxdW9d0LPz7eiICGDqAUB7nG5FB6LPRZlO7fr/+uvV3cdU8s1xebxSQVaKJg7qpu4Wax4Qu+JpRIFEAQBOkZKUoDsHpevOQSyDBJh6AADAqghNPSxYsEC5ublKSUlRXl6e1q9f3677PvzwQyUlJenCCy+03CaJAgAAVkUgUVixYoWmT5+umTNnqrS0VCNHjtTo0aNVXl7e5n01NTWaMGGCfvzjH1tvVCQKAABEhSeffFKTJk3S5MmTNXjwYD311FPKycnRwoUL27zvZz/7me644w4VFBR0qF0ShSiTaHqVYPqnog6z9aVcAIDQCNazHlwul99RX9/6w84aGhq0detWFRUV+Z0vKirSxo0bA8a5dOlS7d69W7Nnz+5wXylmjCKJplf/qk2qV6J+a14ir2EoxWzUI9qgPWaGnjUuikhcPMsBQNwJ0tMjc3Jy/E7Pnj1bc+bMaXF5dXW1PB6PsrKy/M5nZWWpqqqq1Sb+9re/6aGHHtL69euVlNTxH/ckClGiOUm4XBW+c0+bF+vf9KGGqlpDVS2ZiliyAACwbt++fUpP/351jdPZ9j4dhuG/hbhpmi3OSZLH49Edd9yhuXPn6rzzzrMVI4lClBioI8rX91njVdqnfH2rdH3/gJpC7dNr5iAdNNIiESIAxI8gjSikp6f7JQqBZGZmKjExscXowYEDB1qMMkjS0aNHtWXLFpWWluree++VJHm9XpmmqaSkJL3zzju66qqr2hUqNQpR4kujh+ZouOpP+padnCTUKFn/oitIEgAgDIJVo9BeycnJysvLU0lJid/5kpISDR8+vMX16enp+vTTT1VWVuY7pkyZokGDBqmsrEyXXnppu9tmRCGKbDF661HzUs3WR37nPTI0QyO1x+gemcAAACFXXFys8ePHKz8/XwUFBVq0aJHKy8s1ZcoUSdKMGTO0f/9+vfTSS0pISNCQIUP87u/Vq5dSUlJanD8dEoUokmI26ib9rcX5RJm6VV/q0e8KHAEAIRakqQcrxo0bp0OHDmnevHmqrKzUkCFDtGbNGvXv31+SVFlZedo9FTqCRCFKNK9uGKrqVt8v1D5JikiyEMurGniWA4DWROpZD1OnTtXUqVNbfW/ZsmVt3jtnzpxWV1ScDjUKUSJbx3S2anyva5SsZzXMr2bhh6rWGToRifAAIL7E0dMjSRSixF6ju2ZohI7J4StcfMM4V7O/K3A8oFT9QlfqkJEa6VABADGEqYcossPoqYfMkXIrwVe4uNXorX81L1eluqjK6BrZAAEgXkSgRiFSSBSizE6jR4tzpUbLNbQAgNAxvjvs3B8tmHoAAAABMaIAAIBVTD0AAIBAIrU8MhKYegAAAAExogAAgFVMPQAAgDZF0Q97O5h6AAAAATGiAACARfFUzEiiAACAVdQoAACAQOJpRIEaBQAAEBAjCgAAWMXUAwAACISpBwAAADGigDj3dkVZpEMAEI2YegAAAAHFUaLA1AMAAAiIEQUAACyKp2JGEgUAAKxi6gEAAIARBQAALDNMU4bZ8WEBO/eGG4kCAABWxdHUA4kCAAAWxVMxY8hqFL766itNmjRJubm5Sk1N1cCBAzV79mw1NDSEqkkAABBkIRtR2LFjh7xer55//nmdc845+uyzz/TTn/5UtbW1evzxx0PVLAAAocfUg33XXXedrrvuOt/rs88+Wzt37tTChQtJFAAAUS2eph7CWqNQU1OjHj16BHy/vr5e9fX1vtcul0uS5Ha75Xa75Uh1hDxGSXKkJvl9jUX0sYm70RmucEKi0eP0+xqL6GNsCHUfE9xuSZLDEZ6fE/HEMM3wrNHYvXu3Lr74Yj3xxBOaPHlyq9fMmTNHc+fObXF++fLlSktLC3WIAIAoN2bMmJB+vsvlUkZGhi6+/RElJqd0+HM8DSf0v6/OVE1NjdLT04MYYfBZThQC/TA/2ebNm5Wfn+97XVFRoSuvvFJXXnmlXnjhhYD3tTaikJOTo+rqaqWnp2tM94lWQu0wR2qS7l58s5ZMWil3XWNY2gy3eOrj1RfOU1Ji/elviEKNHqfeLZtFH6McfbQvIatUUuhHFJoThbxx9hOFrSuiI1GwPO5877336vbbb2/zmgEDBvj+u6KiQoWFhSooKNCiRYvavM/pdMrpbDks5XA45HA45K5zWw3XFnddY9jbDLd46GNSYr0cSbH5j28z+hgb6GPHJTDlEDKWE4XMzExlZma269r9+/ersLBQeXl5Wrp0qRIS2DEaABADWPVgX0VFhUaNGqV+/frp8ccf18GDB33v9e7dO1TNAgAQFtG0csGOkCUK77zzjnbt2qVdu3apb9++fu+FqX4SAADYFLK5gDvvvFOmabZ6AAAQ1UzT/hElYncRPQAAIcKGSwAAILA4KmZkGQIAAAiIEQUAACwyvE2HnfujBYkCAABWMfUAAADAiAIAAJax6gEAAARmdy+EKNpHgakHAAAQECMKAABYxNQDAAAILI5WPZAoIKq9XVEW8D13o1NvbR0XvmAAIAZRoxBhPcw6PWP+twabh3znzjKPaoH5rvqbNRGMDAAQSPPUg50jWpAoRFAPs06Pa60G6e+ar/UabB7SWeZRPaZ1OldH9JjWkSwAQGfE0yMRDv+kHcrRMUlSFzVqvtarTknK1AlJ0hmq1//VNs3UyEiGCQA4RTwVMzKiEEHPaaj+R719r7uo0ZckSNKX6q75ujQSoQEAIIlEIaLcRqLmqkBb1avFe3uVrgd1hY4ZyRGIDADQJjMIR5QgUYiwXjqufjra6vmcVs4DACKPYkaERXPh4pmqa/Fec83CyashAAAINxKFCPoH7fFLEr5Ud318Ss3COO2MRGgAgLZ4TftHlCBRiKBFGqp31U9SU5LwoK7QHBX4ChxLdabm65JIhggAaE0c1SiwPDKCTMPQ78wfab+66g2d4ytcnGsW6A7t0KsapHqDbxEAIHL4KRRhpmHoP/UDv3NuI1Ev6ocRiggAcDqGbO6jELRIQo9EAQAAq+zursjOjEBwtPXQJwBA6FHMCACARZHaR2HBggXKzc1VSkqK8vLytH79+oDXvv7667rmmmt05plnKj09XQUFBXr77bctt0miAACAVRFY9bBixQpNnz5dM2fOVGlpqUaOHKnRo0ervLy81evXrVuna665RmvWrNHWrVtVWFioG264QaWlpZbaZeoBAACLDNOUYaPOoCP3Pvnkk5o0aZImT54sSXrqqaf09ttva+HChZo/f36L65966im/17/5zW/05ptv6k9/+pMuuuiidrfLiAIAABHicrn8jvr6+lava2ho0NatW1VUVOR3vqioSBs3bmxXW16vV0ePHlWPHj0sxUiiAACAVd4gHJJycnKUkZHhO1obGZCk6upqeTweZWVl+Z3PyspSVVVVu0J+4oknVFtbq9tuu81SV5l6AADAomBNPezbt0/p6em+806ns+37DP8dGEzTbHGuNa+88ormzJmjN998U716tXxicVtIFAAAiJD09HS/RCGQzMxMJSYmthg9OHDgQItRhlOtWLFCkyZN0h//+EddffXVlmNk6gEAAKvCvOohOTlZeXl5Kikp8TtfUlKi4cOHB7zvlVde0Z133qnly5frJz/5ibVGv8OIAgAAVkVgZ8bi4mKNHz9e+fn5Kigo0KJFi1ReXq4pU6ZIkmbMmKH9+/frpZdektSUJEyYMEFPP/20LrvsMt9oRGpqqjIyMtrdLokCAABRYNy4cTp06JDmzZunyspKDRkyRGvWrFH//v0lSZWVlX57Kjz//PNqbGzUPffco3vuucd3fuLEiVq2bFm72yVRAADAIju7Kzbf3xFTp07V1KlTW33v1B/+H3zwQccaOQWJAiKO5zkAiDpx9FAoihkBAEBAjCgAAGCR4W067NwfLUgUAACwKo6mHkgUAACwqoNPgPS7P0pQowAAAAJiRAEAAIsi8ZjpSCFRAADAqjiqUWDqAQAABMSIAgAAVpmS7CxxjJ4BhdCOKNx4443q16+fUlJSlJ2drfHjx6uioiKUTQIAEHLNNQp2jmgR0kShsLBQr732mnbu3KmVK1dq9+7duuWWW0LZJAAACKKQTj3cf//9vv/u37+/HnroIY0dO1Zut1sOhyOUTQMAEDqmbBYzBi2SkAtbjcLhw4f18ssva/jw4QGThPr6etXX1/teu1wuSZLb7W5KLlLDk1w4UpP8vsaiztRHd6MzJJ/b6HH6fY1F9DE20Ef7EtxuSQrfL6FxtOrBMM3QRvvggw/qmWee0fHjx3XZZZfpz3/+s3r27NnqtXPmzNHcuXNbnF++fLnS0tJCGSYAIAaMGTMmpJ/vcrmUkZGhq4Y9qKTEjic9jZ56vffJb1VTU6P09PQgRhh8lhOFQD/MT7Z582bl5+dLkqqrq3X48GF9/fXXmjt3rjIyMvTnP/9ZhmG0uK+1EYWcnBxVV1crPT1dY7pPtBJqhzlSk3T34pu1ZNJKuesaw9JmuHWmPq7auS0kn9vocerdslm6+sJ5SkqsP/0NUYg+xgb6aF9CVqmk0I8o+BKFC4KQKHwaHYmC5XHne++9V7fffnub1wwYMMD335mZmcrMzNR5552nwYMHKycnR5s2bVJBQUGL+5xOp5zOlv/jHQ6HHA6H3HVuq+Ha4q5rDHub4dYZ+uhICu0/jEmJ9SFvI9LoY2ygjx2XEOa6N3ZmbEPzD/6OaB68OHnUAACAqBNHNQohq2T7+OOP9fHHH2vEiBE644wztGfPHs2aNUsDBw5sdTQBAAB0PiFLFFJTU/X6669r9uzZqq2tVXZ2tq677jq9+uqrrU4vIHa9XVEW6RAAILgYUbDvggsu0HvvvReqjwcAIHLiKFHgoVAAACCgyO+2AwBAtPFKarnK39r9UYJEAQAAi+JpeSRTDwAAICBGFAAAsCqOihlJFAAAsMprSoaNH/be6EkUmHoAAAABMaIAAIBVTD0AAIDAbCYKIlEAACB2xdGIAjUKAAAgIEYUYBsPfQIQd7ymbE0fRNGqBxIFAACsMr1Nh537owRTDwAAICBGFAAAsCqOihlJFAAAsCqOahSYegAAAAFF1YhCifePYWnH7XZrzZo1evPIi3I4HGFpM9zioY8JbrekNUrIKlUCfYxa9DE2xFwfmXoAAAABmbKZKAQtkpDr1ImC+d03weVyhbVdt9ut48ePy+Vyxexv2/QxNtDH2EAfg6tbt24yDCOkbcSTTp0oHD16VJKUk5MT4UgAANGipqZG6enpoW2EqYfOoU+fPtq3b1/Ys0OXy6WcnBzt27cv9H/YIoQ+xgb6GBvoY3B169YtpJ8vSfJ6JdnYNMkbPRsudepEISEhQX379o1Y++np6TH7l7YZfYwN9DE20McoEkcjCiyPBAAAAXXqEQUAADqlOBpRIFFohdPp1OzZs+V0OiMdSsjQx9hAH2MDfYxCcbQzo2GaUZTWAAAQQS6XSxkZGbq6x11KSkju8Oc0ehv07uGl4VmhYRMjCgAAWGSaXpk2HhVt595wI1EAAMAq07Q3fRBFg/msegAAAAExogAAgFWmzWJGRhRix4033qh+/fopJSVF2dnZGj9+vCoqKiIdVtB89dVXmjRpknJzc5WamqqBAwdq9uzZamhoiHRoQfXII49o+PDhSktLU/fu3SMdTlAsWLBAubm5SklJUV5entavXx/pkIJq3bp1uuGGG9SnTx8ZhqE33ngj0iEF1fz58/WjH/1I3bp1U69evTR27Fjt3Lkz0mEF1cKFCzV06FDfJksFBQX661//GumwgsPrtX9ECRKF0ygsLNRrr72mnTt3auXKldq9e7duueWWSIcVNDt27JDX69Xzzz+vzz//XP/+7/+u5557Tg8//HCkQwuqhoYG3Xrrrfr5z38e6VCCYsWKFZo+fbpmzpyp0tJSjRw5UqNHj1Z5eXmkQwua2tpaDRs2TM8880ykQwmJtWvX6p577tGmTZtUUlKixsZGFRUVqba2NtKhBU3fvn316KOPasuWLdqyZYuuuuoqjRkzRp9//nmkQ4MFLI+0aPXq1Ro7dqzq6+tj9ilvjz32mBYuXKg9e/ZEOpSgW7ZsmaZPn64jR45EOhRbLr30Ul188cVauHCh79zgwYM1duxYzZ8/P4KRhYZhGFq1apXGjh0b6VBC5uDBg+rVq5fWrl2rK664ItLhhEyPHj302GOPadKkSZEOpUOal0f+uOsdSjJsLI80G/Tfx5ZHxfJIRhQsOHz4sF5++WUNHz48ZpMEqenJaz169Ih0GAigoaFBW7duVVFRkd/5oqIibdy4MUJRwa6amhpJitm/ex6PR6+++qpqa2tVUFAQ6XBsM71e20e0IFFohwcffFBdunRRz549VV5erjfffDPSIYXM7t279Yc//EFTpkyJdCgIoLq6Wh6PR1lZWX7ns7KyVFVVFaGoYIdpmiouLtaIESM0ZMiQSIcTVJ9++qm6du0qp9OpKVOmaNWqVfrBD34Q6bDsa97C2c4RJeIyUZgzZ44Mw2jz2LJli+/6Bx54QKWlpXrnnXeUmJioCRMmqLPP2FjtoyRVVFTouuuu06233qrJkydHKPL260gfY8mpj143TTOsj2NH8Nx7773atm2bXnnllUiHEnSDBg1SWVmZNm3apJ///OeaOHGivvjii0iHBQvicnnkvffeq9tvv73NawYMGOD778zMTGVmZuq8887T4MGDlZOTo02bNnXq4TOrfayoqFBhYaEKCgq0aNGiEEcXHFb7GCsyMzOVmJjYYvTgwIEDLUYZ0PlNmzZNq1ev1rp169S3b99IhxN0ycnJOueccyRJ+fn52rx5s55++mk9//zzEY7MJq8pGfGxPDIuE4XmH/wd0TySUF9fH8yQgs5KH/fv36/CwkLl5eVp6dKlSkiIjoEmO9/HaJacnKy8vDyVlJTopptu8p0vKSnRmDFjIhgZrDBNU9OmTdOqVav0wQcfKDc3N9IhhYVpmp3+3892MU1JNuoMSBRiw8cff6yPP/5YI0aM0BlnnKE9e/Zo1qxZGjhwYKceTbCioqJCo0aNUr9+/fT444/r4MGDvvd69+4dwciCq7y8XIcPH1Z5ebk8Ho/KysokSeecc466du0a2eA6oLi4WOPHj1d+fr5vFKi8vDymakuOHTumXbt2+V7v3btXZWVl6tGjh/r16xfByILjnnvu0fLly/Xmm2+qW7duvhGijIwMpaamRji64Hj44Yc1evRo5eTk6OjRo3r11Vf1wQcf6K233op0aLCARKENqampev311zV79mzV1tYqOztb1113nV599dWYeVTqO++8o127dmnXrl0thj07ex2GFbNmzdKLL77oe33RRRdJkt5//32NGjUqQlF13Lhx43To0CHNmzdPlZWVGjJkiNasWaP+/ftHOrSg2bJliwoLC32vi4uLJUkTJ07UsmXLIhRV8DQvbT31z9/SpUt15513hj+gEPj22281fvx4VVZWKiMjQ0OHDtVbb72la665JtKh2WZ6TZk2ph6i6d9X9lEAAKCdmvdRKEz8RyUZHV8m32i69b7ndfZRAAAAwWN16/a1a9cqLy9PKSkpOvvss/Xcc89ZbpNEAQAAi0yvafuwyurW7Xv37tX111+vkSNHqrS0VA8//LDuu+8+rVy50lK7TD0AANBOzVMPozTG9tTDB3rT0tSD1a3bH3zwQa1evVrbt2/3nZsyZYo++eQTffTRR+2OlREFAAAsapRbjaaNQ25JTYnHyUegpaMd2br9o48+anH9tddeqy1btsjtdre7r6x6AACgnZKTk9W7d29tqFpj+7O6du2qnJwcv3OzZ8/WnDlzWlzbka3bq6qqWr2+sbFR1dXVys7OblecJAoAALRTSkqK9u7dq4aGBtuf1dq266dbem916/bWrm/tfFtIFAAAsCAlJUUpKSlhbbMjW7f37t271euTkpLUs2fPdrdNjQIAAJ3cyVu3n6ykpETDhw9v9Z6CgoIW17/zzjvKz8+Xw9H+QkwSBQAAokBxcbFeeOEFLVmyRNu3b9f999/vt3X7jBkzNGHCBN/1U6ZM0ddff63i4mJt375dS5Ys0eLFi/XLX/7SUrtMPQAAEAVOt3V7ZWWl354Kubm5WrNmje6//349++yz6tOnj37/+9/r5ptvttQu+ygAAICAmHoAAAABkSgAAICASBQAAEBAJAoAACAgEgUAABAQiQIAAAiIRAEAAAREogAAAAIiUQAAAAGRKAAAgIBIFAAAQED/D/+E+xaa4VF2AAAAAElFTkSuQmCC\n"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<__main__.Sequential at 0x2c3cc6107c0>"
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "MaWfgC7Qe3ar",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "source": [
    "# TEST 4: try calling these methods that train with a simple dataset\n",
    "def nn_tanh_test():\n",
    "    np.random.seed(0)\n",
    "    nn = Sequential([Linear(2, 3), Tanh(), Linear(3, 2), SoftMax()], NLL())\n",
    "    X, Y = super_simple_separable()\n",
    "    nn.sgd(X, Y, iters=1, lrate=0.005)\n",
    "    return [np.vstack([nn.modules[0].W, nn.modules[0].W0.T]).tolist(),\n",
    "            np.vstack([nn.modules[2].W, nn.modules[2].W0.T]).tolist()]\n",
    "\n",
    "\n",
    "def nn_relu_test():\n",
    "    np.random.seed(0)\n",
    "    nn = Sequential([Linear(2, 3), ReLU(), Linear(3, 2), SoftMax()], NLL())\n",
    "    X, Y = super_simple_separable()\n",
    "    nn.sgd(X, Y, iters=2, lrate=0.005)\n",
    "    return [np.vstack([nn.modules[0].W, nn.modules[0].W0.T]).tolist(),\n",
    "            np.vstack([nn.modules[2].W, nn.modules[2].W0.T]).tolist()]\n",
    "\n",
    "\n",
    "def nn_pred_test():\n",
    "    np.random.seed(0)\n",
    "    nn = Sequential([Linear(2, 3), ReLU(), Linear(3, 2), SoftMax()], NLL())\n",
    "    X, Y = super_simple_separable()\n",
    "    nn.sgd(X, Y, iters=1, lrate=0.005)\n",
    "    Ypred = nn.forward(X)\n",
    "    return nn.modules[-1].class_fun(Ypred).tolist(), [nn.loss.forward(Ypred, Y)]\n"
   ],
   "execution_count": 52,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "_dx-zM2y3R0z",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "source": [
    "nn_tanh_test()\n",
    "\n",
    "# Expected output:\n",
    "# '''\n",
    "# [[[1.2473733761848262, 0.2829538808226157, 0.6924193292712828],\n",
    "#   [1.5845507770278007, 1.320562932207846, -0.6901721567010647],\n",
    "#   [-8.47337764291184e-12, 2.6227368810847106e-09, 0.00017353185263155828]],\n",
    "#  [[0.544808855557535, -0.08366117689965663],\n",
    "#   [-0.06331837550937104, 0.24078409926389266],\n",
    "#   [0.08677202043839037, 0.8360167748667923],\n",
    "#   [-0.0037249480614718, 0.0037249480614718]]]\n",
    "# '''"
   ],
   "execution_count": 53,
   "outputs": [
    {
     "data": {
      "text/plain": "[[[1.2473733761848262, 0.2829538808226157, 0.6924193292712828],\n  [1.5845507770278007, 1.320562932207846, -0.6901721567010647],\n  [-8.47337764291184e-12, 2.6227368810847102e-09, 0.00017353185263155828]],\n [[0.544808855557535, -0.08366117689965663],\n  [-0.06331837550937103, 0.24078409926389266],\n  [0.08677202043839037, 0.8360167748667923],\n  [-0.0037249480614717995, 0.0037249480614718]]]"
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "WmYT9IWk3TQL",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "source": [
    "nn_relu_test()\n",
    "\n",
    "# Expected output:\n",
    "# '''\n",
    "# [[[1.2421914999646917, 0.2851239946607419, 0.6905003767490479],\n",
    "#   [1.5695659964519526, 1.3273884281993562, -0.6920877418422037],\n",
    "#   [-0.0027754917572235106, 0.001212351486908601, -0.0005239629389906042]],\n",
    "#  [[0.501769700845158, -0.040622022187279644],\n",
    "#   [-0.09260786974986723, 0.27007359350438886],\n",
    "#   [0.08364438851530624, 0.8391444067898763],\n",
    "#   [-0.004252310922204504, 0.004252310922204505]]]\n",
    "# '''"
   ],
   "execution_count": 54,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration = 1 \tAcc = 0.5 \tLoss = 4.658260117134536\n"
     ]
    },
    {
     "data": {
      "text/plain": "[[[1.2421914999646917, 0.2851239946607419, 0.6905003767490479],\n  [1.5695659964519526, 1.3273884281993562, -0.6920877418422037],\n  [-0.002775491757223511, 0.001212351486908601, -0.0005239629389906042]],\n [[0.501769700845158, -0.04062202218727964],\n  [-0.09260786974986725, 0.27007359350438886],\n  [0.08364438851530624, 0.8391444067898763],\n  [-0.004252310922204505, 0.004252310922204505]]]"
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "uo_woDFh3a2v",
    "outputId": "dc8b59c0-3ae0-447c-ac5c-4a25d010fc46",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "source": [
    "nn_pred_test()\n",
    "\n",
    "# Expected output:\n",
    "# '''\n",
    "# ([0, 0, 0, 0], [8.56575061835767])\n",
    "# '''"
   ],
   "execution_count": 55,
   "outputs": [
    {
     "data": {
      "text/plain": "([0, 0, 0, 0], [8.565750618357669])"
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ]
  }
 ]
}