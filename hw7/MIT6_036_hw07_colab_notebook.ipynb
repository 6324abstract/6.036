{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  }
 },
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_xIaEwCD406A",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "#MIT 6.036 Spring 2019: Homework 7#\n",
    "\n",
    "This colab notebook provides code and a framework for problem 2 of [the homework](https://openlearninglibrary.mit.edu/courses/course-v1:MITx+6.036+1T2019/courseware/Week7/week7_homework/).  You can work out your solutions here, then submit your results back on the homework page when ready.\n",
    "\n",
    "## <section>**Setup**</section>\n",
    "\n",
    "First, download the code distribution for this homework that contains test cases and helper functions.\n",
    "\n",
    "Run the next code block to download and import the code for this lab.\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "2YM-_zLf9Bp-",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 408
    },
    "outputId": "849dbafd-f0a6-49a4-fcd8-b1eb935e85e9",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "source": [
    "from   hw7code import *\n",
    "import numpy as np\n",
    "import  modules_disp as disp"
   ],
   "execution_count": 44,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xFxhrJ5XDlvb",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# 2) Implementing Neural Networks\n",
    "\n",
    "This homework considers neural networks with multiple layers. Each layer has multiple inputs and outputs, and can be broken down into two parts:\n",
    "\n",
    "<br>\n",
    "A linear module that implements a linear transformation:     $ z_j = (\\sum^{m}_{i=1} x_i W_{i,j}) + {W_0}_jz$\n",
    "\n",
    "specified by a weight matrix $W$ and a bias vector $W_0$. The output is $[z_1, \\ldots, z_n]^T$\n",
    "\n",
    "<br>\n",
    "An activation module that applies an activation function to the outputs of the linear module for some activation function $f$, such as Tanh or ReLU in the hidden layers or Softmax (see below) at the output layer. We write the output as: $[f(z_1), \\ldots, f(z_m)]^T$, although technically, for some activation functions such as softmax, each output will depend on all the $z_i$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MjQgtwPHj08n",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "We'll use the modular implementation that we guided you through in the previous problem, which leads to clean code. The basic framework for SGD training is given below. We can construct a network and train it as follows:\n",
    "\n",
    "```\n",
    "# build a 3-layer network\n",
    "net = Sequential([Linear(2,3), Tanh(),\n",
    "                  Linear(3,3), Tanh(),\n",
    "    \t          Linear(3,2), SoftMax()])\n",
    "# train the network on data and labels\n",
    "net.sgd(X, Y)\n",
    "```\n",
    "Please fill in any unimplemented methods below:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cEwpgsbnho9K",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Linear Modules: ##\n",
    "Each linear module has a forward method that takes in a batch of activations A (from the previous layer) and returns a batch of pre-activations Z.\n",
    "\n",
    "Each linear module has a backward method that takes in dLdZ and returns dLdA. This module also computes and stores dLdW and dLdW0, the gradients with respect to the weights."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "-VsYLAxCfy7U",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "source": [
    "class Linear(Module):\n",
    "    def __init__(self, m, n):\n",
    "        self.m, self.n = (m, n)  # (in size, out size)\n",
    "        self.W0 = np.zeros([self.n, 1])  # (n x 1)\n",
    "        self.W = np.random.normal(0, 1.0 * m ** (-.5), [m, n])  # (m x n)\n",
    "\n",
    "    def forward(self, A):\n",
    "        self.A = A   # (m x b)  Hint: make sure you understand what b stands for\n",
    "        return  self.W.T@A+self.W0 # Your code (n x b)\n",
    "\n",
    "    def backward(self, dLdZ):  # dLdZ is (n x b), uses stored self.A\n",
    "        self.dLdW  = self.A @ dLdZ.T  # Your code\n",
    "        self.dLdW0 =  dLdZ@np.ones((self.A.shape[1],1)) # Your code\n",
    "        return self.W@dLdZ        # Your code: return dLdA (m x b)\n",
    "\n",
    "    def sgd_step(self, lrate):  # Gradient descent step\n",
    "        self.W  = self.W-lrate*self.dLdW # Your code\n",
    "        self.W0 = self.W0-lrate*self.dLdW0 # Your code"
   ],
   "execution_count": 45,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cqZ7_kZYr5s5",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    " You are encouraged to make your own tests for each module. A unit test method and an example test case are given below for your reference:"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "aY3yePY0r4eA",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "source": [
    "np.random.seed(0)\n",
    "\n",
    "# data\n",
    "X, Y = super_simple_separable()\n",
    "\n",
    "# module\n",
    "linear_1 = Linear(2, 3)\n",
    "\n",
    "#hyperparameters\n",
    "lrate = 0.005\n",
    "\n",
    "# test case\n",
    "# forward\n",
    "z_1 = linear_1.forward(X)\n",
    "exp_z_1 =  np.array([[10.41750064, 6.91122168, 20.73366505, 22.8912344],\n",
    "                     [7.16872235, 3.48998746, 10.46996239, 9.9982611],\n",
    "                     [-2.07105455, 0.69413716, 2.08241149, 4.84966811]])\n",
    "unit_test(\"linear_forward\", exp_z_1, z_1)\n",
    "\n",
    "# backward\n",
    "dL_dz1 = np.array([[1.69467553e-09, -1.33530535e-06, 0.00000000e+00, -0.00000000e+00],\n",
    "                                     [-5.24547376e-07, 5.82459519e-04, -3.84805202e-10, 1.47943038e-09],\n",
    "                                     [-3.47063705e-02, 2.55611604e-01, -1.83538094e-02, 1.11838432e-04]])\n",
    "exp_dLdX = np.array([[-2.40194628e-02, 1.77064845e-01, -1.27021626e-02, 7.74006953e-05],\n",
    "                                    [2.39827939e-02, -1.75870737e-01, 1.26832126e-02, -7.72828555e-05]])\n",
    "dLdX = linear_1.backward(dL_dz1)\n",
    "unit_test(\"linear_backward\", exp_dLdX, dLdX)\n",
    "\n",
    "# sgd step\n",
    "linear_1.sgd_step(lrate)\n",
    "exp_linear_1_W = np.array([[1.2473734,  0.28294514,  0.68940437],\n",
    "                           [1.58455079, 1.32055711, -0.69218045]]),\n",
    "unit_test(\"linear_sgd_step_W\",  exp_linear_1_W,  linear_1.W)\n",
    "\n",
    "exp_linear_1_W0 = np.array([[6.66805339e-09],\n",
    "                            [-2.90968033e-06],\n",
    "                            [-1.01331631e-03]]),\n",
    "unit_test(\"linear_sgd_step_W0\", exp_linear_1_W0, linear_1.W0)\n"
   ],
   "execution_count": 46,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "linear_forward: OK\n",
      "linear_backward: OK\n",
      "linear_sgd_step_W: OK\n",
      "linear_sgd_step_W0: OK\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2ETL01mPsBz4",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "The following datasets are defined for your use:\n",
    "*  `super_simple_separable_through_origin()`\n",
    "*  `super_simple_separable()`\n",
    "*  `xor()`\n",
    "*  `xor_more()`\n",
    "*  `hard()`\n",
    "\n",
    "Further, a plotting function is defined for your usage in modules_disp.py, and can be called in the colab notebook as `disp.plot_nn()`.\n",
    "```\n",
    "def plot_nn(X, Y, nn):\n",
    "    \"\"\" Plot output of nn vs. data \"\"\"\n",
    "    def predict(x):\n",
    "        return nn.modules[-1].class_fun(nn.forward(x))[0]\n",
    "    xmin, ymin = np.min(X, axis=1)-1\n",
    "    xmax, ymax = np.max(X, axis=1)+1\n",
    "    nax = plot_objective_2d(lambda x: predict(x), xmin, xmax, ymin, ymax)\n",
    "    plot_data(X, Y, nax)\n",
    "    plt.show()```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4s70beWJh09h",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Activation functions: ##\n",
    "Each activation module has a forward method that takes in a batch of pre-activations Z and returns a batch of activations A.\n",
    "\n",
    "Each activation module has a backward method that takes in dLdA and returns dLdZ, with the exception of SoftMax, where we assume dLdZ is passed in."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kwaNAtLnhenT",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Tanh: ###"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "ff6eD3dnftiR",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "source": [
    "class Tanh(Module):            # Layer activation\n",
    "    def forward(self, Z):\n",
    "        self.A = np.tanh(Z)\n",
    "        return self.A\n",
    "\n",
    "    def backward(self, dLdA):    # Uses stored self.A\n",
    "        tmp=1-self.A**2\n",
    "        return dLdA*(1-self.A**2) # Your code: return dLdZ (?, b)"
   ],
   "execution_count": 47,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2FW7ocKRhcgY",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### ReLU: ###"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "1fm2KsLUfqdp",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "source": [
    "class ReLU(Module):              # Layer activation\n",
    "    def forward(self, Z):\n",
    "        self.A = np.where(Z>0,Z,0)           # Your code: (?, b)\n",
    "        return self.A\n",
    "\n",
    "    def backward(self, dLdA):    # uses stored self.A\n",
    "        return dLdA*np.where(self.A>0,1,0)      # Your code: return dLdZ (?, b)"
   ],
   "execution_count": 48,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZKtXuTQ0hSNO",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "###SoftMax: ###\n",
    "For `SoftMax.class_fun()`, given the column vector of class probabilities for each point (computed by Softmax), return a vector of the classes (integers) with the highest probability for each point."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "fqK-CJrnfn22",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "source": [
    "class SoftMax(Module):           # Output activation\n",
    "    def forward(self, Z):\n",
    "        ypred=np.exp(Z)\n",
    "        Ypred=ypred/np.sum(np.exp(Z),axis=0)\n",
    "        return Ypred             # Your code: (?, b)\n",
    "\n",
    "    def backward(self, dLdZ):    # Assume that dLdZ is passed in\n",
    "        return dLdZ\n",
    "\n",
    "    def class_fun(self, Ypred):  # Return class indices\n",
    "        return  np.array([np.amax(Ypred,axis=0)])                 # Your code: (1, b)\n"
   ],
   "execution_count": 49,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CZc7HnMSh4fn",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Loss Functions:##\n",
    "Each loss module has a forward method that takes in a batch of predictions Ypred (from the previous layer) and labels Y and returns a scalar loss value.\n",
    "\n",
    "The NLL module has a backward method that returns dLdZ, the gradient with respect to the preactivation to SoftMax (note: not the activation!), since we are always pairing SoftMax activation with NLL loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "l4uy0pHVhNd8",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### NLL: ###"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "17Fb8mimflgb",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "source": [
    "class NLL(Module):       # Loss\n",
    "    def forward(self, Ypred, Y):\n",
    "        self.Ypred = Ypred\n",
    "        self.Y = Y\n",
    "        return -np.sum(Y*np.log(Ypred))    # Your code\n",
    "\n",
    "    def backward(self):  # Use stored self.Ypred, self.Y\n",
    "        return self.Ypred-self.Y      # Your code"
   ],
   "execution_count": 50,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "y1EffzDFkqMX",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Activation and Loss Test Cases: ##\n",
    "Run Test 1 and Test 2 below and compare your outputs with the expected outputs.\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "9DJFzpahkvcD",
    "outputId": "f37fe4f7-9d34-474f-cac3-2183396e7bed",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 180
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "source": [
    "# TEST 1: sgd_test for Tanh activation and SoftMax output\n",
    "np.random.seed(0)\n",
    "sgd_test(Sequential([Linear(2,3), Tanh(), Linear(3,2), SoftMax()], NLL()), test_1_values)"
   ],
   "execution_count": 51,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "linear_1.W: OK\n",
      "linear_1.W0: OK\n",
      "linear_2.W: OK\n",
      "linear_2.W0: OK\n",
      "z_1: OK\n",
      "a_1: OK\n",
      "z_2: OK\n",
      "a_2: OK\n",
      "loss: OK\n",
      "dloss: OK\n",
      "dL_dz2: OK\n",
      "dL_da1: OK\n",
      "dL_dz1: OK\n",
      "dL_dX: OK\n",
      "updated_linear_1.W: OK\n",
      "updated_linear_1.W0: OK\n",
      "updated_linear_2.W: OK\n",
      "updated_linear_2.W0: OK\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "Bd0dXg-Qk05_",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "source": [
    "# TEST 2: sgd_test for ReLU activation and SoftMax output\n",
    "np.random.seed(0)\n",
    "sgd_test(Sequential([Linear(2,3), ReLU(), Linear(3,2), SoftMax()], NLL()), test_2_values)"
   ],
   "execution_count": 52,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "linear_1.W: OK\n",
      "linear_1.W0: OK\n",
      "linear_2.W: OK\n",
      "linear_2.W0: OK\n",
      "z_1: OK\n",
      "a_1: OK\n",
      "z_2: OK\n",
      "a_2: OK\n",
      "loss: OK\n",
      "dloss: OK\n",
      "dL_dz2: OK\n",
      "dL_da1: OK\n",
      "dL_dz1: OK\n",
      "dL_dX: OK\n",
      "updated_linear_1.W: OK\n",
      "updated_linear_1.W0: OK\n",
      "updated_linear_2.W: OK\n",
      "updated_linear_2.W0: OK\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-l5JgBU2iBCZ",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Neural Network: ##"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eXMGcdnXgiF3",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Implement SGD. Randomly pick a data point Xt, Yt by using np.random.randint to choose a random index into the data. Compute the predicted output Ypred for Xt with the forward method. Compute the loss for Ypred relative to Yt. Use the backward method to compute the gradients. Use the sgd_step method to change the weights. Repeat.\n",
    "\n",
    "We will (later) be generalizing SGD to operate on a \"mini-batch\" of data points instead of a single point. You should strive for an implementation of the forward, backward, and `class_fun` methods that works with batches of data. Note that when $b$ is mentioned as part of the shape of a matrix in the code, this $b$ refers to the number of points."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "ejO15Vr7fhKB",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "source": [
    "class Sequential:\n",
    "    def __init__(self, modules, loss):            # List of modules, loss module\n",
    "        self.modules = modules\n",
    "        self.loss = loss\n",
    "\n",
    "    def sgd(self, X, Y, iters=100, lrate=0.005):  # Train\n",
    "        D, N = X.shape\n",
    "        for it in range(iters):\n",
    "            ran_num=np.random.randint(N) # Your code\n",
    "            ypred=self.forward(X[:,ran_num:ran_num+1])\n",
    "            Loss=self.loss.forward(ypred,Y[:,ran_num:ran_num+1])\n",
    "            self.backward(self.loss.backward())\n",
    "            self.sgd_step(lrate)\n",
    "            self.print_accuracy(it,X,Y,Loss)\n",
    "    def forward(self, Xt):                        # Compute Ypred\n",
    "        for m in self.modules: Xt = m.forward(Xt)\n",
    "        return Xt\n",
    "\n",
    "    def backward(self, delta):                    # Update dLdW and dLdW0\n",
    "        # Note reversed list of modules\n",
    "        for m in self.modules[::-1]: delta = m.backward(delta)\n",
    "\n",
    "    def sgd_step(self, lrate):                    # Gradient descent step\n",
    "        for m in self.modules: m.sgd_step(lrate)\n",
    "\n",
    "    def print_accuracy(self, it, X, Y, cur_loss, every=250):\n",
    "        # Utility method to print accuracy on full dataset, should\n",
    "        # improve over time when doing SGD. Also prints current loss,\n",
    "        # which should decrease over time. Call this on each iteration\n",
    "        # of SGD!\n",
    "        if it % every == 1:\n",
    "            cf = self.modules[-1].class_fun\n",
    "            acc = np.mean(cf(self.forward(X)) == cf(Y))\n",
    "            print('Iteration =', it, '\\tAcc =', acc, '\\tLoss =', cur_loss, flush=True)"
   ],
   "execution_count": 76,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HUojaXqphDjh",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Neural Network / SGD Test Cases: ##\n",
    "Use Test 3 and Test 4 to help you debug."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "wmupM8OScodw",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "source": [
    "# TEST 3: you should achieve 100% accuracy on the hard dataset (note that we provided plotting code)\n",
    "X, Y = hard()\n",
    "nn = Sequential([Linear(2, 10), ReLU(), Linear(10, 10), ReLU(), Linear(10,2), SoftMax()], NLL())\n",
    "disp.classify(X, Y, nn, it=100000)"
   ],
   "execution_count": 77,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration = 1 \tAcc = 0.0 \tLoss = 0.5012547168991442\n",
      "Iteration = 251 \tAcc = 0.0 \tLoss = 0.050139361046683505\n",
      "Iteration = 501 \tAcc = 0.0 \tLoss = 0.18555998529789786\n",
      "Iteration = 751 \tAcc = 0.0 \tLoss = 0.1342745222535659\n",
      "Iteration = 1001 \tAcc = 0.0 \tLoss = 0.04863627591590879\n",
      "Iteration = 1251 \tAcc = 0.0 \tLoss = 0.023127883545820127\n",
      "Iteration = 1501 \tAcc = 0.0 \tLoss = 0.4578458785996737\n",
      "Iteration = 1751 \tAcc = 0.0 \tLoss = 0.014700598300892757\n",
      "Iteration = 2001 \tAcc = 0.0 \tLoss = 0.0001715961677330622\n",
      "Iteration = 2251 \tAcc = 0.0 \tLoss = 0.0007536539299220835\n",
      "Iteration = 2501 \tAcc = 0.0 \tLoss = 1.9585795236232344\n",
      "Iteration = 2751 \tAcc = 0.0 \tLoss = 0.21297222169905713\n",
      "Iteration = 3001 \tAcc = 0.0 \tLoss = 0.16306157386532505\n",
      "Iteration = 3251 \tAcc = 0.0 \tLoss = 2.7263678336155082e-05\n",
      "Iteration = 3501 \tAcc = 0.0 \tLoss = 0.0012822936666687932\n",
      "Iteration = 3751 \tAcc = 0.0 \tLoss = 2.2666048583113104e-05\n",
      "Iteration = 4001 \tAcc = 0.0 \tLoss = 0.00014869865621951387\n",
      "Iteration = 4251 \tAcc = 0.0 \tLoss = 0.003307517926971276\n",
      "Iteration = 4501 \tAcc = 0.0 \tLoss = 0.02717004622879144\n",
      "Iteration = 4751 \tAcc = 0.0 \tLoss = 0.41288455908184046\n",
      "Iteration = 5001 \tAcc = 0.0 \tLoss = 0.002906255064803456\n",
      "Iteration = 5251 \tAcc = 0.0 \tLoss = 0.0022410146912875475\n",
      "Iteration = 5501 \tAcc = 0.0 \tLoss = 0.030091711610107676\n",
      "Iteration = 5751 \tAcc = 0.0 \tLoss = 5.0668922859657206e-05\n",
      "Iteration = 6001 \tAcc = 0.0 \tLoss = 0.029850887040515688\n",
      "Iteration = 6251 \tAcc = 0.0 \tLoss = 0.0019515659143143698\n",
      "Iteration = 6501 \tAcc = 0.0 \tLoss = 2.290240513710405e-06\n",
      "Iteration = 6751 \tAcc = 0.0 \tLoss = 0.001719962914656628\n",
      "Iteration = 7001 \tAcc = 0.0 \tLoss = 1.5933845267731622e-06\n",
      "Iteration = 7251 \tAcc = 0.0 \tLoss = 9.915602112609109e-07\n",
      "Iteration = 7501 \tAcc = 0.0 \tLoss = 0.0013536414184688237\n",
      "Iteration = 7751 \tAcc = 0.0 \tLoss = 1.0372939006028947e-08\n",
      "Iteration = 8001 \tAcc = 0.0 \tLoss = 0.003200832744292077\n",
      "Iteration = 8251 \tAcc = 0.0 \tLoss = 0.0008256029140445311\n",
      "Iteration = 8501 \tAcc = 0.0 \tLoss = 4.047167832301125e-06\n",
      "Iteration = 8751 \tAcc = 0.0 \tLoss = 0.4038774489933872\n",
      "Iteration = 9001 \tAcc = 0.0 \tLoss = 0.3197849860451715\n",
      "Iteration = 9251 \tAcc = 0.0 \tLoss = 0.002530457055086274\n",
      "Iteration = 9501 \tAcc = 0.0 \tLoss = 9.814373762948529e-10\n",
      "Iteration = 9751 \tAcc = 0.0 \tLoss = 0.003042602356061788\n",
      "Iteration = 10001 \tAcc = 0.0 \tLoss = 0.2905941520220486\n",
      "Iteration = 10251 \tAcc = 0.0 \tLoss = 0.00045329597102135196\n",
      "Iteration = 10501 \tAcc = 0.0 \tLoss = 1.6669903263487143e-08\n",
      "Iteration = 10751 \tAcc = 0.0 \tLoss = 0.016540004128724136\n",
      "Iteration = 11001 \tAcc = 0.0 \tLoss = 0.20365010410508155\n",
      "Iteration = 11251 \tAcc = 0.0 \tLoss = 7.354854641274634e-09\n",
      "Iteration = 11501 \tAcc = 0.0 \tLoss = 8.443581213191009e-08\n",
      "Iteration = 11751 \tAcc = 0.0 \tLoss = 1.5198942883150863\n",
      "Iteration = 12001 \tAcc = 0.0 \tLoss = 0.09458964313828569\n",
      "Iteration = 12251 \tAcc = 0.0 \tLoss = 0.0032548629889990284\n",
      "Iteration = 12501 \tAcc = 0.0 \tLoss = 0.2178304757249334\n",
      "Iteration = 12751 \tAcc = 0.0 \tLoss = 1.3185107074748226\n",
      "Iteration = 13001 \tAcc = 0.0 \tLoss = 0.0015299317136862624\n",
      "Iteration = 13251 \tAcc = 0.0 \tLoss = 3.174459312508968e-08\n",
      "Iteration = 13501 \tAcc = 0.0 \tLoss = 0.0013144177341492616\n",
      "Iteration = 13751 \tAcc = 0.0 \tLoss = 2.56509591378276e-10\n",
      "Iteration = 14001 \tAcc = 0.0 \tLoss = 1.3119847738796866e-08\n",
      "Iteration = 14251 \tAcc = 0.0 \tLoss = 0.0938581524810182\n",
      "Iteration = 14501 \tAcc = 0.0 \tLoss = 1.1851291670442818\n",
      "Iteration = 14751 \tAcc = 0.0 \tLoss = 4.528651530156181e-05\n",
      "Iteration = 15001 \tAcc = 0.0 \tLoss = 2.434432325358938e-09\n",
      "Iteration = 15251 \tAcc = 0.0 \tLoss = 3.806040712066461e-09\n",
      "Iteration = 15501 \tAcc = 0.0 \tLoss = 0.11295367206673629\n",
      "Iteration = 15751 \tAcc = 0.0 \tLoss = 0.0019413875158071081\n",
      "Iteration = 16001 \tAcc = 0.0 \tLoss = 3.5278633418235326e-05\n",
      "Iteration = 16251 \tAcc = 0.0 \tLoss = 1.9778634287883382e-10\n",
      "Iteration = 16501 \tAcc = 0.0 \tLoss = 0.002375252882579443\n",
      "Iteration = 16751 \tAcc = 0.0 \tLoss = 0.20119057452406672\n",
      "Iteration = 17001 \tAcc = 0.0 \tLoss = 0.0007183951688104015\n",
      "Iteration = 17251 \tAcc = 0.0 \tLoss = 0.0015066970072485624\n",
      "Iteration = 17501 \tAcc = 0.0 \tLoss = 0.21061866681248972\n",
      "Iteration = 17751 \tAcc = 0.0 \tLoss = 0.20727864325313683\n",
      "Iteration = 18001 \tAcc = 0.0 \tLoss = 8.398017271716197e-06\n",
      "Iteration = 18251 \tAcc = 0.0 \tLoss = 1.136496574958673\n",
      "Iteration = 18501 \tAcc = 0.0 \tLoss = 1.1653490198332042\n",
      "Iteration = 18751 \tAcc = 0.0 \tLoss = 0.0006955876873011551\n",
      "Iteration = 19001 \tAcc = 0.0 \tLoss = 4.23228119226312e-11\n",
      "Iteration = 19251 \tAcc = 0.05 \tLoss = 0.004057114714068322\n",
      "Iteration = 19501 \tAcc = 0.05 \tLoss = 1.9887668198728787e-09\n",
      "Iteration = 19751 \tAcc = 0.05 \tLoss = 3.2519873513633906e-09\n",
      "Iteration = 20001 \tAcc = 0.05 \tLoss = 0.08635113816804814\n",
      "Iteration = 20251 \tAcc = 0.05 \tLoss = -0.0\n",
      "Iteration = 20501 \tAcc = 0.05 \tLoss = 0.000638052385399613\n",
      "Iteration = 20751 \tAcc = 0.05 \tLoss = 2.093841383448195e-06\n",
      "Iteration = 21001 \tAcc = 0.05 \tLoss = 0.0007902454704848009\n",
      "Iteration = 21251 \tAcc = 0.05 \tLoss = 1.186631506125508\n",
      "Iteration = 21501 \tAcc = 0.05 \tLoss = 0.3566885152904521\n",
      "Iteration = 21751 \tAcc = 0.05 \tLoss = 0.050508383941868594\n",
      "Iteration = 22001 \tAcc = 0.05 \tLoss = 0.04901322718656671\n",
      "Iteration = 22251 \tAcc = 0.05 \tLoss = 2.2204460492503136e-16\n",
      "Iteration = 22501 \tAcc = 0.05 \tLoss = 1.0680820936432112\n",
      "Iteration = 22751 \tAcc = 0.05 \tLoss = 1.2699211581147406e-06\n",
      "Iteration = 23001 \tAcc = 0.05 \tLoss = 1.2750842037920845e-06\n",
      "Iteration = 23251 \tAcc = 0.05 \tLoss = 0.0002541273196275224\n",
      "Iteration = 23501 \tAcc = 0.1 \tLoss = -0.0\n",
      "Iteration = 23751 \tAcc = 0.1 \tLoss = 0.00022714417117908296\n",
      "Iteration = 24001 \tAcc = 0.1 \tLoss = 2.087219286295316e-14\n",
      "Iteration = 24251 \tAcc = 0.1 \tLoss = 0.06477426175495807\n",
      "Iteration = 24501 \tAcc = 0.1 \tLoss = 0.0001409126799269396\n",
      "Iteration = 24751 \tAcc = 0.1 \tLoss = -0.0\n",
      "Iteration = 25001 \tAcc = 0.1 \tLoss = 0.7129144224393255\n",
      "Iteration = 25251 \tAcc = 0.1 \tLoss = 0.8468179169599112\n",
      "Iteration = 25501 \tAcc = 0.1 \tLoss = 0.8384965044431677\n",
      "Iteration = 25751 \tAcc = 0.1 \tLoss = 0.00013206992128986537\n",
      "Iteration = 26001 \tAcc = 0.1 \tLoss = 0.00018769038997417276\n",
      "Iteration = 26251 \tAcc = 0.1 \tLoss = 3.0465571510008043e-07\n",
      "Iteration = 26501 \tAcc = 0.1 \tLoss = 4.6629367034256685e-15\n",
      "Iteration = 26751 \tAcc = 0.1 \tLoss = 0.036125728588232216\n",
      "Iteration = 27001 \tAcc = 0.1 \tLoss = 3.298694650771706e-12\n",
      "Iteration = 27251 \tAcc = 0.1 \tLoss = 0.011762309965433449\n",
      "Iteration = 27501 \tAcc = 0.1 \tLoss = 2.009614696876015e-12\n",
      "Iteration = 27751 \tAcc = 0.1 \tLoss = -0.0\n",
      "Iteration = 28001 \tAcc = 0.1 \tLoss = 0.06202052462377936\n",
      "Iteration = 28251 \tAcc = 0.1 \tLoss = 1.8602202470149762e-05\n",
      "Iteration = 28501 \tAcc = 0.15 \tLoss = 1.3024936067508272e-07\n",
      "Iteration = 28751 \tAcc = 0.1 \tLoss = 8.661649641582892e-05\n",
      "Iteration = 29001 \tAcc = 0.15 \tLoss = 0.00436504263972479\n",
      "Iteration = 29251 \tAcc = 0.15 \tLoss = 0.03457810703263516\n",
      "Iteration = 29501 \tAcc = 0.15 \tLoss = 2.2204460492503136e-16\n",
      "Iteration = 29751 \tAcc = 0.15 \tLoss = -0.0\n",
      "Iteration = 30001 \tAcc = 0.15 \tLoss = 0.01931632222377939\n",
      "Iteration = 30251 \tAcc = 0.15 \tLoss = 8.317664314610956e-08\n",
      "Iteration = 30501 \tAcc = 0.15 \tLoss = -0.0\n",
      "Iteration = 30751 \tAcc = 0.2 \tLoss = 0.8703447586488088\n",
      "Iteration = 31001 \tAcc = 0.15 \tLoss = 1.0161040325786759e-07\n",
      "Iteration = 31251 \tAcc = 0.2 \tLoss = 4.5277757236030583e-08\n",
      "Iteration = 31501 \tAcc = 0.2 \tLoss = 0.0015155881815597\n",
      "Iteration = 31751 \tAcc = 0.2 \tLoss = -0.0\n",
      "Iteration = 32001 \tAcc = 0.2 \tLoss = -0.0\n",
      "Iteration = 32251 \tAcc = 0.2 \tLoss = 0.7626412801308805\n",
      "Iteration = 32501 \tAcc = 0.2 \tLoss = -0.0\n",
      "Iteration = 32751 \tAcc = 0.2 \tLoss = 0.002137614931246748\n",
      "Iteration = 33001 \tAcc = 0.2 \tLoss = -0.0\n",
      "Iteration = 33251 \tAcc = 0.2 \tLoss = -0.0\n",
      "Iteration = 33501 \tAcc = 0.2 \tLoss = 0.0023595635625361013\n",
      "Iteration = 33751 \tAcc = 0.2 \tLoss = 0.0013298017124049486\n",
      "Iteration = 34001 \tAcc = 0.2 \tLoss = 0.025755986855170887\n",
      "Iteration = 34251 \tAcc = 0.2 \tLoss = -0.0\n",
      "Iteration = 34501 \tAcc = 0.2 \tLoss = -0.0\n",
      "Iteration = 34751 \tAcc = 0.2 \tLoss = 0.015226890531050087\n",
      "Iteration = 35001 \tAcc = 0.2 \tLoss = 0.0018865005434950594\n",
      "Iteration = 35251 \tAcc = 0.2 \tLoss = 6.596993831731205e-06\n",
      "Iteration = 35501 \tAcc = 0.2 \tLoss = -0.0\n",
      "Iteration = 35751 \tAcc = 0.2 \tLoss = -0.0\n",
      "Iteration = 36001 \tAcc = 0.2 \tLoss = 0.02745535004252481\n",
      "Iteration = 36251 \tAcc = 0.2 \tLoss = 0.0012327139308956527\n",
      "Iteration = 36501 \tAcc = 0.2 \tLoss = 0.0015399820597491916\n",
      "Iteration = 36751 \tAcc = 0.2 \tLoss = 3.274215174854125e-08\n",
      "Iteration = 37001 \tAcc = 0.2 \tLoss = 0.002309149518481443\n",
      "Iteration = 37251 \tAcc = 0.2 \tLoss = 0.020483371962018753\n",
      "Iteration = 37501 \tAcc = 0.2 \tLoss = 0.013282880170734188\n",
      "Iteration = 37751 \tAcc = 0.2 \tLoss = -0.0\n",
      "Iteration = 38001 \tAcc = 0.2 \tLoss = 0.045600957698215934\n",
      "Iteration = 38251 \tAcc = 0.2 \tLoss = -0.0\n",
      "Iteration = 38501 \tAcc = 0.2 \tLoss = 0.016111308038713336\n",
      "Iteration = 38751 \tAcc = 0.2 \tLoss = -0.0\n",
      "Iteration = 39001 \tAcc = 0.2 \tLoss = 0.0012139097355755141\n",
      "Iteration = 39251 \tAcc = 0.2 \tLoss = 0.2413465201487364\n",
      "Iteration = 39501 \tAcc = 0.2 \tLoss = 0.001680145281397446\n",
      "Iteration = 39751 \tAcc = 0.2 \tLoss = 0.0009753177527727076\n",
      "Iteration = 40001 \tAcc = 0.2 \tLoss = 3.637534382262608e-06\n",
      "Iteration = 40251 \tAcc = 0.2 \tLoss = 0.019897886338937094\n",
      "Iteration = 40501 \tAcc = 0.2 \tLoss = -0.0\n",
      "Iteration = 40751 \tAcc = 0.2 \tLoss = 0.0010977110157756166\n",
      "Iteration = 41001 \tAcc = 0.2 \tLoss = 6.743741499500409e-06\n",
      "Iteration = 41251 \tAcc = 0.2 \tLoss = 0.8380996208246284\n",
      "Iteration = 41501 \tAcc = 0.2 \tLoss = 0.2552876011402583\n",
      "Iteration = 41751 \tAcc = 0.2 \tLoss = -0.0\n",
      "Iteration = 42001 \tAcc = 0.2 \tLoss = -0.0\n",
      "Iteration = 42251 \tAcc = 0.2 \tLoss = 0.5197514413763372\n",
      "Iteration = 42501 \tAcc = 0.25 \tLoss = 0.011163221212467225\n",
      "Iteration = 42751 \tAcc = 0.2 \tLoss = 2.024172107091653e-06\n",
      "Iteration = 43001 \tAcc = 0.25 \tLoss = 8.931049051333182e-09\n",
      "Iteration = 43251 \tAcc = 0.25 \tLoss = 0.5809358763745519\n",
      "Iteration = 43501 \tAcc = 0.25 \tLoss = -0.0\n",
      "Iteration = 43751 \tAcc = 0.25 \tLoss = 4.771204109857063e-09\n",
      "Iteration = 44001 \tAcc = 0.25 \tLoss = 0.0014839645896912064\n",
      "Iteration = 44251 \tAcc = 0.25 \tLoss = 0.0021166364247842346\n",
      "Iteration = 44501 \tAcc = 0.25 \tLoss = 2.2997717090488604e-09\n",
      "Iteration = 44751 \tAcc = 0.25 \tLoss = 0.010085687344711445\n",
      "Iteration = 45001 \tAcc = 0.25 \tLoss = 0.009208236302247808\n",
      "Iteration = 45251 \tAcc = 0.25 \tLoss = 1.7004966040782672e-05\n",
      "Iteration = 45501 \tAcc = 0.25 \tLoss = -0.0\n",
      "Iteration = 45751 \tAcc = 0.25 \tLoss = 0.004815560086312801\n",
      "Iteration = 46001 \tAcc = 0.25 \tLoss = 1.8718192569021956e-09\n",
      "Iteration = 46251 \tAcc = 0.25 \tLoss = 0.0019482635296678332\n",
      "Iteration = 46501 \tAcc = 0.25 \tLoss = 0.0021139732102301773\n",
      "Iteration = 46751 \tAcc = 0.25 \tLoss = -0.0\n",
      "Iteration = 47001 \tAcc = 0.25 \tLoss = 0.26588937761080694\n",
      "Iteration = 47251 \tAcc = 0.25 \tLoss = 1.3059152961146202e-05\n",
      "Iteration = 47501 \tAcc = 0.25 \tLoss = 0.004223034740625738\n",
      "Iteration = 47751 \tAcc = 0.25 \tLoss = -0.0\n",
      "Iteration = 48001 \tAcc = 0.25 \tLoss = 0.0011191852036352907\n",
      "Iteration = 48251 \tAcc = 0.25 \tLoss = -0.0\n",
      "Iteration = 48501 \tAcc = 0.25 \tLoss = 0.0012057703836764627\n",
      "Iteration = 48751 \tAcc = 0.25 \tLoss = 0.01019069968789173\n",
      "Iteration = 49001 \tAcc = 0.25 \tLoss = 0.009729696802956721\n",
      "Iteration = 49251 \tAcc = 0.25 \tLoss = 2.9672596646049294e-09\n",
      "Iteration = 49501 \tAcc = 0.25 \tLoss = -0.0\n",
      "Iteration = 49751 \tAcc = 0.25 \tLoss = 0.0001939096809030507\n",
      "Iteration = 50001 \tAcc = 0.25 \tLoss = 0.007611057199033362\n",
      "Iteration = 50251 \tAcc = 0.25 \tLoss = 8.8798657355573e-10\n",
      "Iteration = 50501 \tAcc = 0.25 \tLoss = 0.006234518337208872\n",
      "Iteration = 50751 \tAcc = 0.25 \tLoss = 1.0042461336473765\n",
      "Iteration = 51001 \tAcc = 0.25 \tLoss = 0.011339389796674083\n",
      "Iteration = 51251 \tAcc = 0.25 \tLoss = 0.2595730339556514\n",
      "Iteration = 51501 \tAcc = 0.25 \tLoss = -0.0\n",
      "Iteration = 51751 \tAcc = 0.25 \tLoss = 6.452219871583175e-10\n",
      "Iteration = 52001 \tAcc = 0.25 \tLoss = 0.0021527058830066548\n",
      "Iteration = 52251 \tAcc = 0.25 \tLoss = 1.756251779781564e-06\n",
      "Iteration = 52501 \tAcc = 0.25 \tLoss = 8.203263627305093e-10\n",
      "Iteration = 52751 \tAcc = 0.25 \tLoss = 0.18765543397672743\n",
      "Iteration = 53001 \tAcc = 0.25 \tLoss = 0.0018674322932242507\n",
      "Iteration = 53251 \tAcc = 0.25 \tLoss = -0.0\n",
      "Iteration = 53501 \tAcc = 0.25 \tLoss = 8.442304715114471e-07\n",
      "Iteration = 53751 \tAcc = 0.25 \tLoss = -0.0\n",
      "Iteration = 54001 \tAcc = 0.25 \tLoss = 1.6473917781379782e-06\n",
      "Iteration = 54251 \tAcc = 0.25 \tLoss = 0.0009069265151209334\n",
      "Iteration = 54501 \tAcc = 0.25 \tLoss = 0.0011143451729384714\n",
      "Iteration = 54751 \tAcc = 0.25 \tLoss = 1.1449021835386526e-06\n",
      "Iteration = 55001 \tAcc = 0.25 \tLoss = 0.34792486652253474\n",
      "Iteration = 55251 \tAcc = 0.25 \tLoss = -0.0\n",
      "Iteration = 55501 \tAcc = 0.25 \tLoss = 2.749989125683896e-10\n",
      "Iteration = 55751 \tAcc = 0.25 \tLoss = 0.14452866985349527\n",
      "Iteration = 56001 \tAcc = 0.25 \tLoss = -0.0\n",
      "Iteration = 56251 \tAcc = 0.25 \tLoss = 9.754383066921208e-07\n",
      "Iteration = 56501 \tAcc = 0.25 \tLoss = 2.1925598782361247e-06\n",
      "Iteration = 56751 \tAcc = 0.25 \tLoss = 3.494922130087387e-10\n",
      "Iteration = 57001 \tAcc = 0.25 \tLoss = 6.400866615186839e-06\n",
      "Iteration = 57251 \tAcc = 0.25 \tLoss = 7.110865232503837e-10\n",
      "Iteration = 57501 \tAcc = 0.25 \tLoss = 2.0634893795798557e-10\n",
      "Iteration = 57751 \tAcc = 0.25 \tLoss = 7.051519370903499e-10\n",
      "Iteration = 58001 \tAcc = 0.25 \tLoss = 0.33543339484456236\n",
      "Iteration = 58251 \tAcc = 0.25 \tLoss = -0.0\n",
      "Iteration = 58501 \tAcc = 0.25 \tLoss = 0.002580326812703586\n",
      "Iteration = 58751 \tAcc = 0.25 \tLoss = 2.751234795917868e-10\n",
      "Iteration = 59001 \tAcc = 0.25 \tLoss = 2.5068258583205317e-10\n",
      "Iteration = 59251 \tAcc = 0.25 \tLoss = 5.04503772219142e-10\n",
      "Iteration = 59501 \tAcc = 0.25 \tLoss = 0.0018302006145321094\n",
      "Iteration = 59751 \tAcc = 0.25 \tLoss = -0.0\n",
      "Iteration = 60001 \tAcc = 0.25 \tLoss = 0.001833494760866646\n",
      "Iteration = 60251 \tAcc = 0.25 \tLoss = 0.0050529146280428715\n",
      "Iteration = 60501 \tAcc = 0.25 \tLoss = 0.005323749674713262\n",
      "Iteration = 60751 \tAcc = 0.25 \tLoss = 1.9213797221764966e-10\n",
      "Iteration = 61001 \tAcc = 0.25 \tLoss = 0.008883126780361421\n",
      "Iteration = 61251 \tAcc = 0.25 \tLoss = 0.00604977226914436\n",
      "Iteration = 61501 \tAcc = 0.25 \tLoss = -0.0\n",
      "Iteration = 61751 \tAcc = 0.25 \tLoss = 0.006509720003134859\n",
      "Iteration = 62001 \tAcc = 0.25 \tLoss = 1.6070811349647884e-10\n",
      "Iteration = 62251 \tAcc = 0.25 \tLoss = 0.0008756510926920069\n",
      "Iteration = 62501 \tAcc = 0.25 \tLoss = 0.3709928302211277\n",
      "Iteration = 62751 \tAcc = 0.25 \tLoss = -0.0\n",
      "Iteration = 63001 \tAcc = 0.25 \tLoss = 6.31524148657371e-07\n",
      "Iteration = 63251 \tAcc = 0.25 \tLoss = 3.401523773852511e-07\n",
      "Iteration = 63501 \tAcc = 0.25 \tLoss = 0.003020440621343141\n",
      "Iteration = 63751 \tAcc = 0.25 \tLoss = 1.1270284706124172e-10\n",
      "Iteration = 64001 \tAcc = 0.25 \tLoss = -0.0\n",
      "Iteration = 64251 \tAcc = 0.25 \tLoss = 0.0011711753854683332\n",
      "Iteration = 64501 \tAcc = 0.25 \tLoss = 7.550070595620783e-07\n",
      "Iteration = 64751 \tAcc = 0.25 \tLoss = 5.919154055964203e-11\n",
      "Iteration = 65001 \tAcc = 0.25 \tLoss = 0.0021191064458460315\n",
      "Iteration = 65251 \tAcc = 0.25 \tLoss = 3.235628902638009e-06\n",
      "Iteration = 65501 \tAcc = 0.25 \tLoss = 0.004731807851505544\n",
      "Iteration = 65751 \tAcc = 0.25 \tLoss = -0.0\n",
      "Iteration = 66001 \tAcc = 0.25 \tLoss = 0.003124642852428191\n",
      "Iteration = 66251 \tAcc = 0.25 \tLoss = 0.12916259521960233\n",
      "Iteration = 66501 \tAcc = 0.25 \tLoss = 0.31831596357589365\n",
      "Iteration = 66751 \tAcc = 0.25 \tLoss = 1.7973156298206413e-10\n",
      "Iteration = 67001 \tAcc = 0.25 \tLoss = -0.0\n",
      "Iteration = 67251 \tAcc = 0.25 \tLoss = 0.0007021770531432353\n",
      "Iteration = 67501 \tAcc = 0.25 \tLoss = -0.0\n",
      "Iteration = 67751 \tAcc = 0.25 \tLoss = -0.0\n",
      "Iteration = 68001 \tAcc = 0.25 \tLoss = -0.0\n",
      "Iteration = 68251 \tAcc = 0.25 \tLoss = -0.0\n",
      "Iteration = 68501 \tAcc = 0.25 \tLoss = 5.7915783281970575e-11\n",
      "Iteration = 68751 \tAcc = 0.25 \tLoss = 0.0019051252298469509\n",
      "Iteration = 69001 \tAcc = 0.25 \tLoss = 0.1653919519436001\n",
      "Iteration = 69251 \tAcc = 0.25 \tLoss = 0.09461740676787359\n",
      "Iteration = 69501 \tAcc = 0.25 \tLoss = 0.001801863368137322\n",
      "Iteration = 69751 \tAcc = 0.25 \tLoss = 3.259903458338997e-11\n",
      "Iteration = 70001 \tAcc = 0.25 \tLoss = 4.481637283604795e-11\n",
      "Iteration = 70251 \tAcc = 0.25 \tLoss = 3.093048039962782e-11\n",
      "Iteration = 70501 \tAcc = 0.25 \tLoss = -0.0\n",
      "Iteration = 70751 \tAcc = 0.25 \tLoss = 0.0030224847633646035\n",
      "Iteration = 71001 \tAcc = 0.25 \tLoss = 1.1136092049122695e-10\n",
      "Iteration = 71251 \tAcc = 0.25 \tLoss = 0.005148761962444715\n",
      "Iteration = 71501 \tAcc = 0.25 \tLoss = 0.0022624533367127833\n",
      "Iteration = 71751 \tAcc = 0.25 \tLoss = 2.594435777359199e-11\n",
      "Iteration = 72001 \tAcc = 0.25 \tLoss = 1.3134715667289383e-07\n",
      "Iteration = 72251 \tAcc = 0.25 \tLoss = 0.13739824838808712\n",
      "Iteration = 72501 \tAcc = 0.25 \tLoss = 1.5331943657608637e-07\n",
      "Iteration = 72751 \tAcc = 0.25 \tLoss = -0.0\n",
      "Iteration = 73001 \tAcc = 0.25 \tLoss = 0.0016093351135030743\n",
      "Iteration = 73251 \tAcc = 0.25 \tLoss = 0.24960670308112978\n",
      "Iteration = 73501 \tAcc = 0.25 \tLoss = 0.0017841639865112694\n",
      "Iteration = 73751 \tAcc = 0.25 \tLoss = -0.0\n",
      "Iteration = 74001 \tAcc = 0.25 \tLoss = -0.0\n",
      "Iteration = 74251 \tAcc = 0.25 \tLoss = 1.7976221779045791e-06\n",
      "Iteration = 74501 \tAcc = 0.25 \tLoss = 3.2573417172934787e-07\n",
      "Iteration = 74751 \tAcc = 0.25 \tLoss = -0.0\n",
      "Iteration = 75001 \tAcc = 0.25 \tLoss = 7.292544346437232e-11\n",
      "Iteration = 75251 \tAcc = 0.25 \tLoss = 0.0004980788997327762\n",
      "Iteration = 75501 \tAcc = 0.25 \tLoss = 0.001252966788805415\n",
      "Iteration = 75751 \tAcc = 0.25 \tLoss = 0.0016078352338679516\n",
      "Iteration = 76001 \tAcc = 0.25 \tLoss = -0.0\n",
      "Iteration = 76251 \tAcc = 0.25 \tLoss = 0.24548375442741469\n",
      "Iteration = 76501 \tAcc = 0.25 \tLoss = 0.06811102162572935\n",
      "Iteration = 76751 \tAcc = 0.25 \tLoss = 0.0005453639083143606\n",
      "Iteration = 77001 \tAcc = 0.25 \tLoss = 0.00278747877563834\n",
      "Iteration = 77251 \tAcc = 0.25 \tLoss = -0.0\n",
      "Iteration = 77501 \tAcc = 0.25 \tLoss = -0.0\n",
      "Iteration = 77751 \tAcc = 0.25 \tLoss = 8.110774703271363e-08\n",
      "Iteration = 78001 \tAcc = 0.25 \tLoss = 8.866233030011943e-08\n",
      "Iteration = 78251 \tAcc = 0.25 \tLoss = 1.8391283732926278e-07\n",
      "Iteration = 78501 \tAcc = 0.25 \tLoss = 0.0005482024210890068\n",
      "Iteration = 78751 \tAcc = 0.25 \tLoss = 1.206818806231107e-06\n",
      "Iteration = 79001 \tAcc = 0.25 \tLoss = 8.082350626764542e-08\n",
      "Iteration = 79251 \tAcc = 0.25 \tLoss = 0.001773029220548023\n",
      "Iteration = 79501 \tAcc = 0.25 \tLoss = 8.360028397055193e-08\n",
      "Iteration = 79751 \tAcc = 0.25 \tLoss = 1.0665468508420906e-11\n",
      "Iteration = 80001 \tAcc = 0.25 \tLoss = 0.004015678025273937\n",
      "Iteration = 80251 \tAcc = 0.25 \tLoss = 0.0015977259028356792\n",
      "Iteration = 80501 \tAcc = 0.25 \tLoss = 0.003185972246701429\n",
      "Iteration = 80751 \tAcc = 0.25 \tLoss = 8.918310534551188e-12\n",
      "Iteration = 81001 \tAcc = 0.25 \tLoss = 0.0004681201905991855\n",
      "Iteration = 81251 \tAcc = 0.25 \tLoss = 4.213351889692462e-11\n",
      "Iteration = 81501 \tAcc = 0.25 \tLoss = 0.0010661719853508078\n",
      "Iteration = 81751 \tAcc = 0.25 \tLoss = 8.372635917943206e-12\n",
      "Iteration = 82001 \tAcc = 0.25 \tLoss = 1.2739665661618227e-06\n",
      "Iteration = 82251 \tAcc = 0.25 \tLoss = 3.917055568868455e-11\n",
      "Iteration = 82501 \tAcc = 0.25 \tLoss = 0.0014805681299134602\n",
      "Iteration = 82751 \tAcc = 0.25 \tLoss = 0.0015200570099321309\n",
      "Iteration = 83001 \tAcc = 0.25 \tLoss = -0.0\n",
      "Iteration = 83251 \tAcc = 0.25 \tLoss = 0.0004792824283848649\n",
      "Iteration = 83501 \tAcc = 0.25 \tLoss = -0.0\n",
      "Iteration = 83751 \tAcc = 0.25 \tLoss = -0.0\n",
      "Iteration = 84001 \tAcc = 0.25 \tLoss = 0.08033138031263096\n",
      "Iteration = 84251 \tAcc = 0.25 \tLoss = 1.0012757904017398e-06\n",
      "Iteration = 84501 \tAcc = 0.25 \tLoss = 0.031356559054951084\n",
      "Iteration = 84751 \tAcc = 0.25 \tLoss = -0.0\n",
      "Iteration = 85001 \tAcc = 0.25 \tLoss = 1.499590522064576e-07\n",
      "Iteration = 85251 \tAcc = 0.25 \tLoss = 0.0426990877274588\n",
      "Iteration = 85501 \tAcc = 0.25 \tLoss = 0.0015730691599223341\n",
      "Iteration = 85751 \tAcc = 0.25 \tLoss = 5.6721581375107745e-08\n",
      "Iteration = 86001 \tAcc = 0.25 \tLoss = 0.001163990483496646\n",
      "Iteration = 86251 \tAcc = 0.25 \tLoss = -0.0\n",
      "Iteration = 86501 \tAcc = 0.25 \tLoss = -0.0\n",
      "Iteration = 86751 \tAcc = 0.25 \tLoss = 2.758415718100723e-11\n",
      "Iteration = 87001 \tAcc = 0.25 \tLoss = 5.648814749308751e-12\n",
      "Iteration = 87251 \tAcc = 0.25 \tLoss = 0.0014712784329551094\n",
      "Iteration = 87501 \tAcc = 0.25 \tLoss = 4.477812809767565e-08\n",
      "Iteration = 87751 \tAcc = 0.25 \tLoss = 2.696165513108293e-11\n",
      "Iteration = 88001 \tAcc = 0.25 \tLoss = -0.0\n",
      "Iteration = 88251 \tAcc = 0.25 \tLoss = 0.0031473890047146167\n",
      "Iteration = 88501 \tAcc = 0.25 \tLoss = 8.30602253646514e-12\n",
      "Iteration = 88751 \tAcc = 0.25 \tLoss = -0.0\n",
      "Iteration = 89001 \tAcc = 0.25 \tLoss = 0.05529347911376066\n",
      "Iteration = 89251 \tAcc = 0.25 \tLoss = 0.000915292296067545\n",
      "Iteration = 89501 \tAcc = 0.25 \tLoss = -0.0\n",
      "Iteration = 89751 \tAcc = 0.25 \tLoss = 4.8745452119310925e-12\n",
      "Iteration = 90001 \tAcc = 0.25 \tLoss = -0.0\n",
      "Iteration = 90251 \tAcc = 0.25 \tLoss = -0.0\n",
      "Iteration = 90501 \tAcc = 0.25 \tLoss = 9.050895887129472e-08\n",
      "Iteration = 90751 \tAcc = 0.25 \tLoss = -0.0\n",
      "Iteration = 91001 \tAcc = 0.25 \tLoss = 2.3413382344291558e-11\n",
      "Iteration = 91251 \tAcc = 0.25 \tLoss = 0.0005725115834672635\n",
      "Iteration = 91501 \tAcc = 0.25 \tLoss = 0.0027046111740545173\n",
      "Iteration = 91751 \tAcc = 0.25 \tLoss = 0.06624796733153429\n",
      "Iteration = 92001 \tAcc = 0.25 \tLoss = 0.0006547072927208046\n",
      "Iteration = 92251 \tAcc = 0.25 \tLoss = -0.0\n",
      "Iteration = 92501 \tAcc = 0.25 \tLoss = 3.259893030915022e-08\n",
      "Iteration = 92751 \tAcc = 0.25 \tLoss = 2.0485391161383364e-11\n",
      "Iteration = 93001 \tAcc = 0.25 \tLoss = -0.0\n",
      "Iteration = 93251 \tAcc = 0.25 \tLoss = 0.00037484075908513604\n",
      "Iteration = 93501 \tAcc = 0.25 \tLoss = 0.029315877969289885\n",
      "Iteration = 93751 \tAcc = 0.25 \tLoss = 3.771316592356306e-12\n",
      "Iteration = 94001 \tAcc = 0.25 \tLoss = -0.0\n",
      "Iteration = 94251 \tAcc = 0.25 \tLoss = -0.0\n",
      "Iteration = 94501 \tAcc = 0.25 \tLoss = 0.0013858338296287378\n",
      "Iteration = 94751 \tAcc = 0.25 \tLoss = 0.002734908932733141\n",
      "Iteration = 95001 \tAcc = 0.25 \tLoss = 2.981365032662013e-08\n",
      "Iteration = 95251 \tAcc = 0.25 \tLoss = -0.0\n",
      "Iteration = 95501 \tAcc = 0.25 \tLoss = 2.8810929279898957e-08\n",
      "Iteration = 95751 \tAcc = 0.25 \tLoss = 0.0023685962810047964\n",
      "Iteration = 96001 \tAcc = 0.25 \tLoss = 2.5547540964887004e-08\n",
      "Iteration = 96251 \tAcc = 0.25 \tLoss = 2.780461177761726e-08\n",
      "Iteration = 96501 \tAcc = 0.25 \tLoss = -0.0\n",
      "Iteration = 96751 \tAcc = 0.25 \tLoss = 2.684286740051406e-08\n",
      "Iteration = 97001 \tAcc = 0.25 \tLoss = 3.1828983893029266e-12\n",
      "Iteration = 97251 \tAcc = 0.25 \tLoss = -0.0\n",
      "Iteration = 97501 \tAcc = 0.25 \tLoss = 2.276997982775144e-08\n",
      "Iteration = 97751 \tAcc = 0.25 \tLoss = 2.3915210431521733e-08\n",
      "Iteration = 98001 \tAcc = 0.25 \tLoss = -0.0\n",
      "Iteration = 98251 \tAcc = 0.25 \tLoss = 4.853450974463112e-12\n",
      "Iteration = 98501 \tAcc = 0.25 \tLoss = 0.0004398213081317378\n",
      "Iteration = 98751 \tAcc = 0.25 \tLoss = 1.557798334784676e-11\n",
      "Iteration = 99001 \tAcc = 0.25 \tLoss = -0.0\n",
      "Iteration = 99251 \tAcc = 0.25 \tLoss = 5.45353228289077e-07\n",
      "Iteration = 99501 \tAcc = 0.25 \tLoss = 0.04580275826163122\n",
      "Iteration = 99751 \tAcc = 0.25 \tLoss = -0.0\n",
      "-3.46493986 -3.41956036 3.39710997 2.0597278\n"
     ]
    },
    {
     "data": {
      "text/plain": "<Figure size 640x480 with 2 Axes>",
      "image/png": "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\n"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<__main__.Sequential at 0x2d835083430>"
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "MaWfgC7Qe3ar",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "source": [
    "# TEST 4: try calling these methods that train with a simple dataset\n",
    "def nn_tanh_test():\n",
    "    np.random.seed(0)\n",
    "    nn = Sequential([Linear(2, 3), Tanh(), Linear(3, 2), SoftMax()], NLL())\n",
    "    X, Y = super_simple_separable()\n",
    "    nn.sgd(X, Y, iters=1, lrate=0.005)\n",
    "    return [np.vstack([nn.modules[0].W, nn.modules[0].W0.T]).tolist(),\n",
    "            np.vstack([nn.modules[2].W, nn.modules[2].W0.T]).tolist()]\n",
    "\n",
    "\n",
    "def nn_relu_test():\n",
    "    np.random.seed(0)\n",
    "    nn = Sequential([Linear(2, 3), ReLU(), Linear(3, 2), SoftMax()], NLL())\n",
    "    X, Y = super_simple_separable()\n",
    "    nn.sgd(X, Y, iters=2, lrate=0.005)\n",
    "    return [np.vstack([nn.modules[0].W, nn.modules[0].W0.T]).tolist(),\n",
    "            np.vstack([nn.modules[2].W, nn.modules[2].W0.T]).tolist()]\n",
    "\n",
    "\n",
    "def nn_pred_test():\n",
    "    np.random.seed(0)\n",
    "    nn = Sequential([Linear(2, 3), ReLU(), Linear(3, 2), SoftMax()], NLL())\n",
    "    X, Y = super_simple_separable()\n",
    "    nn.sgd(X, Y, iters=1, lrate=0.005)\n",
    "    Ypred = nn.forward(X)\n",
    "    return nn.modules[-1].class_fun(Ypred).tolist(), [nn.loss.forward(Ypred, Y)]\n"
   ],
   "execution_count": 59,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "_dx-zM2y3R0z",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "source": [
    "nn_tanh_test()\n",
    "\n",
    "# Expected output:\n",
    "# '''\n",
    "# [[[1.2473733761848262, 0.2829538808226157, 0.6924193292712828],\n",
    "#   [1.5845507770278007, 1.320562932207846, -0.6901721567010647],\n",
    "#   [-8.47337764291184e-12, 2.6227368810847106e-09, 0.00017353185263155828]],\n",
    "#  [[0.544808855557535, -0.08366117689965663],\n",
    "#   [-0.06331837550937104, 0.24078409926389266],\n",
    "#   [0.08677202043839037, 0.8360167748667923],\n",
    "#   [-0.0037249480614718, 0.0037249480614718]]]\n",
    "# '''"
   ],
   "execution_count": 60,
   "outputs": [
    {
     "data": {
      "text/plain": "[[[1.2473733761848262, 0.2829538808226157, 0.6924193292712828],\n  [1.5845507770278007, 1.320562932207846, -0.6901721567010647],\n  [-8.47337764291184e-12, 2.6227368810847102e-09, 0.00017353185263155828]],\n [[0.544808855557535, -0.08366117689965663],\n  [-0.06331837550937103, 0.24078409926389266],\n  [0.08677202043839037, 0.8360167748667923],\n  [-0.0037249480614717995, 0.0037249480614718]]]"
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "WmYT9IWk3TQL",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "source": [
    "nn_relu_test()\n",
    "\n",
    "# Expected output:\n",
    "# '''\n",
    "# [[[1.2421914999646917, 0.2851239946607419, 0.6905003767490479],\n",
    "#   [1.5695659964519526, 1.3273884281993562, -0.6920877418422037],\n",
    "#   [-0.0027754917572235106, 0.001212351486908601, -0.0005239629389906042]],\n",
    "#  [[0.501769700845158, -0.040622022187279644],\n",
    "#   [-0.09260786974986723, 0.27007359350438886],\n",
    "#   [0.08364438851530624, 0.8391444067898763],\n",
    "#   [-0.004252310922204504, 0.004252310922204505]]]\n",
    "# '''"
   ],
   "execution_count": 61,
   "outputs": [
    {
     "data": {
      "text/plain": "[[[1.2421914999646917, 0.2851239946607419, 0.6905003767490479],\n  [1.5695659964519526, 1.3273884281993562, -0.6920877418422037],\n  [-0.002775491757223511, 0.001212351486908601, -0.0005239629389906042]],\n [[0.501769700845158, -0.04062202218727964],\n  [-0.09260786974986725, 0.27007359350438886],\n  [0.08364438851530624, 0.8391444067898763],\n  [-0.004252310922204505, 0.004252310922204505]]]"
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "uo_woDFh3a2v",
    "outputId": "dc8b59c0-3ae0-447c-ac5c-4a25d010fc46",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "source": [
    "nn_pred_test()\n",
    "\n",
    "# Expected output:\n",
    "# '''\n",
    "# ([0, 0, 0, 0], [8.56575061835767])\n",
    "# '''"
   ],
   "execution_count": 62,
   "outputs": [
    {
     "data": {
      "text/plain": "([[0.9448095487370077,\n   0.8614700508946521,\n   0.9959576192817937,\n   0.9912893277461089]],\n [8.565750618357669])"
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ]
  }
 ]
}